{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC2.5: Dense and Sparse Matrices\n",
    "\n",
    "This section is not necessarily for Marcov chain Monte Carlo itself, but the use of Sparse matrices is important to speed up quantum Monte Carlo simulations.\n",
    "\n",
    "## Dense matrix\n",
    "\n",
    "To use dense matrices, LinearAlgebra provides wrapper to BLAS/LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend Intel MKL instead of OpenBLAS. You can check whether MKL is used by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":mkl"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLAS.vendor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Array is called Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array{Int64, 1} == Vector{Int64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D Array is called Matrix, which I later call dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array{Float64, 2} == Matrix{Float64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix and Array of Array are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = [1 2\n",
    "                 3 4]\n",
    "array =[[1, 2], [3, 4]]\n",
    "matrix == array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's support on 3-rank tensors is limited, but still we can define and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×2 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0  0.0\n",
       " 0.0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = zeros(Float64, 2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, matrices are stored columnwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000024 seconds (7 allocations: 78.375 KiB)\n",
      "  0.000412 seconds (7 allocations: 78.375 KiB)\n"
     ]
    }
   ],
   "source": [
    "mat = ones(10000, 10000)\n",
    "mat[:, 1]\n",
    "mat[1, :]\n",
    "@time mat[:, 1];\n",
    "@time mat[1, :];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, vertical vectors are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 17.0\n",
       " 39.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1.0 2.0; 3.0 4.0] * [5.0; 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS/LAPACK\n",
    "\n",
    "Dense matrices are not memory-efficient, but support BLAS/LAPACK. The most important BLAS operation in quantum Monte Carlo is rank-1 update (or other finite-rank updates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       "  0.0431439    0.0888779   …  -0.232161     0.0334541  -0.10434  \n",
       " -0.235971    -0.401377        0.271148    -0.0722883  -0.195389 \n",
       " -0.0387278   -0.128991        0.0114505   -0.0752756  -0.100264 \n",
       " -0.0289467    0.121712        0.0573061    0.139086    0.0810516\n",
       " -0.212922    -0.0187286       0.233624     0.135971   -0.149635 \n",
       " -0.0580575   -0.176289    …   0.129684     0.0379991  -0.0129432\n",
       "  0.142148     0.141984       -0.263332     0.0531205   0.0427841\n",
       "  0.109876     0.0526541      -0.0992558    0.0143842   0.112669 \n",
       "  0.136653     0.00726805     -0.178598    -0.0400401   0.0482636\n",
       " -0.126976    -0.137025        0.146843    -0.0466192  -0.0871268\n",
       "  0.0941911    0.231223    …  -0.0192677    0.0992502   0.188856 \n",
       " -0.0315682   -0.144275        0.17446      0.0830541  -0.0187862\n",
       " -0.187044    -0.063006        0.0813068    0.0908393  -0.159311 \n",
       "  ⋮                        ⋱                                     \n",
       " -0.0551023    0.129739        0.161075     0.064586    0.088966 \n",
       " -0.0340744   -0.107915       -0.00200205   0.0111678  -0.0730561\n",
       "  0.00186624   0.0466384   …   0.0164247    0.0131588  -0.104912 \n",
       "  0.379537     0.346054       -0.616078     0.027222    0.0878188\n",
       "  0.231179     0.0807727      -0.266694    -0.0401123   0.152077 \n",
       "  0.0549114    0.0161505      -0.0229935   -0.0670675   0.0829392\n",
       "  0.263258     0.242884       -0.394778     0.0494628  -0.0849586\n",
       "  0.0682608   -0.0261475   …  -0.148224    -0.119383   -0.0434909\n",
       "  0.256253     0.208756       -0.343059     0.030659    0.0506311\n",
       " -0.111245    -0.145685        0.13988     -0.0739381   0.0124254\n",
       "  0.0456461    0.0582449      -0.0674311    0.0244256  -0.0373438\n",
       " -0.0246867    0.152204       -0.0344665   -0.019695    0.0514183"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(Float64, 1000, 1000)\n",
    "Ainv = inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the Sherman-Morrison formula!\n",
    "$$\\left( A+\\vec{u} \\vec{v}^T \\right)^{-1} = A^{-1} - \\frac{A^{-1} \\vec{u} \\vec{v}^TA^{-1}}{1 + \\vec{v}^T A^{-1} \\vec{u}}$$\n",
    "Of course, the vectors are vertical. I define $B = A+\\vec{u} \\vec{v}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       " 0.832968  0.652178  0.724346  1.19486   …  0.334528  0.730802  0.928351\n",
       " 1.3461    0.45736   0.662928  0.968384     0.905952  0.555805  0.772106\n",
       " 0.249889  0.121346  0.439088  0.448305     0.254457  0.653032  0.805448\n",
       " 0.820503  0.398808  0.436075  0.477687     0.677882  0.453538  0.764802\n",
       " 0.740269  0.468934  0.632645  0.661285     0.874208  0.650122  0.982307\n",
       " 0.343132  0.569241  0.648693  0.789917  …  0.251079  0.903195  0.706866\n",
       " 1.55833   1.14814   0.659995  1.12276      1.25377   0.93118   0.467273\n",
       " 0.128137  0.821006  0.674959  0.817371     0.1994    0.958172  0.126384\n",
       " 0.963283  0.966691  0.981831  1.23097      0.33287   0.949716  1.14575 \n",
       " 0.810313  1.36004   1.44107   1.53647      0.960035  0.39253   1.19298 \n",
       " 1.1866    0.86983   0.953029  0.340716  …  0.176998  0.710034  1.26138 \n",
       " 0.306116  0.604526  0.444104  0.938056     0.951188  0.475478  0.612429\n",
       " 0.972492  0.578318  0.667652  0.819561     1.05324   0.497566  1.24237 \n",
       " ⋮                                       ⋱                              \n",
       " 0.808986  1.07691   1.13444   0.539359     1.17091   0.274977  0.696279\n",
       " 1.20768   0.983134  1.2647    0.761914     0.727971  0.744881  1.22855 \n",
       " 0.573417  0.950551  0.814177  1.03373   …  0.659967  0.778508  0.558734\n",
       " 0.679733  0.308921  0.359299  1.02293      0.869369  0.500483  0.905708\n",
       " 1.12601   0.576073  1.0328    0.249823     0.72329   0.64027   0.248529\n",
       " 0.529275  0.486078  1.06655   1.01964      0.739558  0.825652  0.549801\n",
       " 0.956668  0.867212  0.862953  1.16474      0.559364  0.345657  0.692899\n",
       " 0.478792  0.368361  0.621234  0.690737  …  0.405726  0.74276   0.399495\n",
       " 1.62902   0.552994  1.00908   0.949644     1.20074   0.231941  1.01247 \n",
       " 0.281471  0.344576  0.355726  0.324736     0.691802  0.919354  0.801872\n",
       " 0.446131  0.156838  0.929907  0.216713     0.594973  0.993126  0.199563\n",
       " 1.42853   1.19178   0.621288  1.06006      0.307289  0.96949   0.751583"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = rand(Float64, 1000)\n",
    "v = rand(Float64, 1000)\n",
    "B = copy(A)\n",
    "BLAS.ger!(1.0, u, v, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to copy the matrix first because most BLAS operations are destructive. The rank-1 update is a BLAS-2 function, so it is faster than the BLAS-3 function, matrix inversion. That's why we use the Sherman-Morrison formula to calculate $B^{-1}$ when we already know $A^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       "  0.0207023    0.013859     0.0422636  …   0.00714479  -0.133266 \n",
       " -0.114238     0.00555833   0.0145399      0.0704248   -0.0384843\n",
       "  0.00567708   0.0194481   -0.0264304     -0.0232178   -0.0430294\n",
       " -0.04895      0.0548441   -0.0803093      0.115635     0.055269 \n",
       " -0.115136     0.308156     0.273267       0.25061     -0.0235963\n",
       " -0.022912    -0.0588034    0.0558867  …   0.0792017    0.0323565\n",
       "  0.0822957   -0.058091    -0.0287768     -0.0170463   -0.0343602\n",
       "  0.0368055   -0.191608    -0.0344167     -0.0712792    0.0184869\n",
       "  0.0599814   -0.249034    -0.0483716     -0.129926    -0.0505605\n",
       " -0.0423401    0.1459       0.22952        0.0526034    0.0219626\n",
       "  0.0452352    0.067571    -0.0780428  …   0.041857     0.125756 \n",
       " -0.027322    -0.130081    -0.0556808      0.0880321   -0.0133131\n",
       " -0.171071    -0.00960869  -0.0462855      0.109566    -0.138722 \n",
       "  ⋮                                    ⋱                         \n",
       " -0.0670159    0.0899132   -0.0324655      0.0506192    0.0736103\n",
       "  0.00999004   0.0393858    0.0272129      0.0628264   -0.0162606\n",
       "  0.0337452    0.153205     0.025532   …   0.0505319   -0.0638222\n",
       "  0.196999    -0.26414     -0.143298      -0.186774    -0.147458 \n",
       "  0.0933485   -0.379973    -0.259841      -0.201697    -0.0255753\n",
       "  0.0179937   -0.107259    -0.123812      -0.110348     0.0353553\n",
       "  0.161937    -0.0958195   -0.0521835     -0.0693211   -0.215554 \n",
       "  0.0645617   -0.0385131   -0.0567013  …  -0.123719    -0.0482588\n",
       "  0.139724    -0.180782    -0.0999672     -0.105953    -0.0995655\n",
       " -0.0506711    0.0568046    0.0351037     -0.00292442   0.0905007\n",
       "  0.0386538    0.0348707    0.111426       0.0162282   -0.0463563\n",
       " -0.0322414    0.12695      0.0653695     -0.0285517    0.041681 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Binv = copy(Ainv)\n",
    "BLAS.ger!(-1.0 / (1.0 + v' * Ainv * u), Ainv * u, (v' * Ainv)', Binv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.424705419107949e-11"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(B * Binv - I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating rank-1 updates accumulates some error, so sometimes you have to refresh the updated matrix \"from scratch.\" (Be careful because sometimes it is not really from scratch.)\n",
    "\n",
    "As for LAPACK, most Julia functions on linear algebra are just wrappers of LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Array{Complex{Float64},1}:\n",
       "    500.2148852665727 + 0.0im                \n",
       "   -6.098333785458043 + 7.017296010747078im  \n",
       "   -6.098333785458043 - 7.017296010747078im  \n",
       "     -8.0972931651208 + 4.207552150579319im  \n",
       "     -8.0972931651208 - 4.207552150579319im  \n",
       "   -8.807360262124128 + 2.2185591531094784im \n",
       "   -8.807360262124128 - 2.2185591531094784im \n",
       "    -8.35892169465259 + 3.411774904994613im  \n",
       "    -8.35892169465259 - 3.411774904994613im  \n",
       "   -9.047722150615531 + 0.27918731766378635im\n",
       "   -9.047722150615531 - 0.27918731766378635im\n",
       "   -7.543040464081789 + 4.76467779877881im   \n",
       "   -7.543040464081789 - 4.76467779877881im   \n",
       "                      ⋮                      \n",
       "  -1.1729019206274498 + 0.29718356345534697im\n",
       "  -1.1729019206274498 - 0.29718356345534697im\n",
       "  0.22307321051946205 + 0.7350647422279357im \n",
       "  0.22307321051946205 - 0.7350647422279357im \n",
       "   -0.817132240547356 + 0.2126900153686736im \n",
       "   -0.817132240547356 - 0.2126900153686736im \n",
       "  0.13457895726107866 + 0.3244186187276212im \n",
       "  0.13457895726107866 - 0.3244186187276212im \n",
       " -0.14988238350551983 + 0.0im                \n",
       "   0.2948922234088385 + 0.0im                \n",
       "  -0.8702047498484011 + 0.0im                \n",
       "  -0.7936892849900651 + 0.0im                "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eigvals is just a wrapper for LAPACK.geevx!, so you can directely call LAPACK.geevx! instead if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse matrix\n",
    "\n",
    "If your program is intensively using sparse matrices, you should use python instead because Julia only supports CSC matrix. Julia's native support for sparse matrices is not strong, so I do not recommend to write a code using multiple types of sparse matrices in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve a tight-binding model on the 2D square lattice in a poor man's way, i.e. in the real space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tuple{Tuple{Int64,Int64},Tuple{Int64,Int64}}[((1, 1), (2, 1)) ((1, 2), (2, 2)) … ((1, 29), (2, 29)) ((1, 30), (2, 30)); ((2, 1), (3, 1)) ((2, 2), (3, 2)) … ((2, 29), (3, 29)) ((2, 30), (3, 30)); … ; ((29, 1), (30, 1)) ((29, 2), (30, 2)) … ((29, 29), (30, 29)) ((29, 30), (30, 30)); ((30, 1), (1, 1)) ((30, 2), (1, 2)) … ((30, 29), (1, 29)) ((30, 30), (1, 30))], Tuple{Tuple{Int64,Int64},Tuple{Int64,Int64}}[((1, 1), (1, 2)) ((1, 2), (1, 3)) … ((1, 29), (1, 30)) ((1, 30), (1, 1)); ((2, 1), (2, 2)) ((2, 2), (2, 3)) … ((2, 29), (2, 30)) ((2, 30), (2, 1)); … ; ((29, 1), (29, 2)) ((29, 2), (29, 3)) … ((29, 29), (29, 30)) ((29, 30), (29, 1)); ((30, 1), (30, 2)) ((30, 2), (30, 3)) … ((30, 29), (30, 30)) ((30, 30), (30, 1))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const L = 30\n",
    "iter1D = 1 : L\n",
    "nnbondx = zip(Iterators.product(iter1D, iter1D), Iterators.product((mod1(i + 1, L) for i in iter1D), iter1D))\n",
    "nnbondy = zip(Iterators.product(iter1D, iter1D), Iterators.product(iter1D, (mod1(i + 1, L) for i in iter1D)))\n",
    "collect(nnbondx), collect(nnbondy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These iterators will generate the 2D square lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Base.Iterators.Zip2{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}}}},typeof(xytoz)}(xytoz, Base.Iterators.Zip2{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}}}}(Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}}((1:30, 1:30)), Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}}}((1:30, Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}(getfield(Main, Symbol(\"##5#6\"))(), 1:30)))))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xytoz(nn::Tuple{Tuple{Int64, Int64}, Tuple{Int64, Int64}}) = (nn[1][2] - 1) * L + nn[1][1], (nn[2][2] - 1) * L + nn[2][1]\n",
    "nnx = Base.Generator(xytoz, nnbondx)\n",
    "nny = Base.Generator(xytoz, nnbondy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base.Generator(f, iter) is same as (f(x) for x in iter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900×900 SparseMatrixCSC{Float64,Int64} with 3600 stored entries:\n",
       "  [2  ,   1]  =  -1.0\n",
       "  [30 ,   1]  =  -1.0\n",
       "  [31 ,   1]  =  -1.0\n",
       "  [871,   1]  =  -1.0\n",
       "  [1  ,   2]  =  -1.0\n",
       "  [3  ,   2]  =  -1.0\n",
       "  [32 ,   2]  =  -1.0\n",
       "  [872,   2]  =  -1.0\n",
       "  [2  ,   3]  =  -1.0\n",
       "  [4  ,   3]  =  -1.0\n",
       "  [33 ,   3]  =  -1.0\n",
       "  [873,   3]  =  -1.0\n",
       "  ⋮\n",
       "  [28 , 898]  =  -1.0\n",
       "  [868, 898]  =  -1.0\n",
       "  [897, 898]  =  -1.0\n",
       "  [899, 898]  =  -1.0\n",
       "  [29 , 899]  =  -1.0\n",
       "  [869, 899]  =  -1.0\n",
       "  [898, 899]  =  -1.0\n",
       "  [900, 899]  =  -1.0\n",
       "  [30 , 900]  =  -1.0\n",
       "  [870, 900]  =  -1.0\n",
       "  [871, 900]  =  -1.0\n",
       "  [899, 900]  =  -1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = L ^ 2\n",
    "H = spzeros(Float64, N, N)\n",
    "for (i, j) in Iterators.flatten((nnx, nny))\n",
    "    H[i, j] = -1.0\n",
    "    H[j, i] = -1.0\n",
    "end\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can rewrite this code by zip, but zip is not efficient here.\n",
    "\n",
    "Most of the operations for sparse matrices are similar to the ones for dense matrices. However, sparse arrays are more memory-efficient when the components of the matrix is almost zero. Especially, if the matrix is sparse enough, it significantly reduces the matrix muliplication cost from $O(N^3)$ to $O(N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000187 seconds (18 allocations: 268.500 KiB)\n",
      "  0.022350 seconds (6 allocations: 6.180 MiB)\n"
     ]
    }
   ],
   "source": [
    "Hdense = Array(H)\n",
    "H * H\n",
    "Hdense * Hdense\n",
    "@time H * H;\n",
    "@time Hdense * Hdense;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eigvals does not support sparse matrices, so the calculation of the whole eigenvalues still costs $O(N^3)$. I will discuss this problem later in MCMC5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900-element Array{Float64,1}:\n",
       " -4.000000000000004 \n",
       " -3.9562952014676074\n",
       " -3.956295201467607 \n",
       " -3.9562952014676056\n",
       " -3.9562952014676043\n",
       " -3.9125904029352245\n",
       " -3.912590402935224 \n",
       " -3.9125904029352214\n",
       " -3.912590402935216 \n",
       " -3.827090915285207 \n",
       " -3.827090915285198 \n",
       " -3.8270909152851953\n",
       " -3.827090915285193 \n",
       "  ⋮                 \n",
       "  3.8270909152851997\n",
       "  3.8270909152852073\n",
       "  3.82709091528521  \n",
       "  3.9125904029352205\n",
       "  3.9125904029352228\n",
       "  3.912590402935223 \n",
       "  3.912590402935234 \n",
       "  3.9562952014676016\n",
       "  3.956295201467605 \n",
       "  3.956295201467612 \n",
       "  3.9562952014676163\n",
       "  3.999999999999999 "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(Hdense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the square (or cubic, etc.) lattice, you can directly begin from a dense matrix. Here's a smart implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900-element Array{Float64,1}:\n",
       " -4.000000000000004 \n",
       " -3.9562952014676074\n",
       " -3.956295201467607 \n",
       " -3.9562952014676056\n",
       " -3.9562952014676043\n",
       " -3.9125904029352245\n",
       " -3.912590402935224 \n",
       " -3.9125904029352214\n",
       " -3.912590402935216 \n",
       " -3.827090915285207 \n",
       " -3.827090915285198 \n",
       " -3.8270909152851953\n",
       " -3.827090915285193 \n",
       "  ⋮                 \n",
       "  3.8270909152851997\n",
       "  3.8270909152852073\n",
       "  3.82709091528521  \n",
       "  3.9125904029352205\n",
       "  3.9125904029352228\n",
       "  3.912590402935223 \n",
       "  3.912590402935234 \n",
       "  3.9562952014676016\n",
       "  3.956295201467605 \n",
       "  3.956295201467612 \n",
       "  3.9562952014676163\n",
       "  3.999999999999999 "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H4d = zeros(Float64, L, L, L, L)\n",
    "for ((i, j), (k, l)) in Iterators.flatten((nnbondx, nnbondy))\n",
    "    H4d[i, j, k, l] = -1.0\n",
    "    H4d[k, l, i, j] = -1.0\n",
    "end\n",
    "H2d = reshape(H4d, N, N)\n",
    "eigvals(H2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block checkerboard decomposition/approximation\n",
    "\n",
    "It is sometimes very useful to approximate a dense matrix by a product of sparse matrices. In the physical models like tight-binding models, block checkerboard docomposition will be a good approximation.\n",
    "\n",
    "~ under construction ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative solvers\n",
    "\n",
    "Iterative solvers, especially conjugate gradient methods are important for hybrid Monte Carlo simulations for lattice gauge theories.\n",
    "\n",
    "~ under construction ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
