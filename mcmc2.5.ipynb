{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC2.5: Dense and Sparse Matrices\n",
    "\n",
    "This section is not necessarily for Marcov chain Monte Carlo itself, but the use of Sparse matrices is important to speed up quantum Monte Carlo simulations.\n",
    "\n",
    "## Dense matrix\n",
    "\n",
    "To use dense matrices, `LinearAlgebra` provides wrapper to BLAS/LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend Intel MKL instead of OpenBLAS. You can check whether MKL is used by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":mkl"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLAS.vendor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Array is called `Vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array{Int64, 1} == Vector{Int64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D Array is called `Matrix`, which I later call dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array{Float64, 2} == Matrix{Float64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matrix` and `Array` of `Array` are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = [1 2\n",
    "                 3 4]\n",
    "array =[[1, 2], [3, 4]]\n",
    "matrix == array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's support on 3-rank tensors is limited, but still we can define and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×2 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0  0.0\n",
       " 0.0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = zeros(Float64, 2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, matrices are stored columnwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000025 seconds (7 allocations: 78.375 KiB)\n",
      "  0.000274 seconds (7 allocations: 78.375 KiB)\n"
     ]
    }
   ],
   "source": [
    "mat = ones(10000, 10000)\n",
    "mat[:, 1]\n",
    "mat[1, :]\n",
    "@time mat[:, 1];\n",
    "@time mat[1, :];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, vertical vectors are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 17.0\n",
       " 39.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1.0 2.0; 3.0 4.0] * [5.0; 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS/LAPACK\n",
    "\n",
    "Dense matrices are not memory-efficient, but support BLAS/LAPACK. The most important BLAS operation in quantum Monte Carlo is rank-1 update (or other finite-rank updates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       " -0.216807    -0.182728   -0.313444   -0.00442697  …   0.246234     0.166771 \n",
       " -0.106858    -0.0511798  -0.0729319  -0.0827161      -0.0443618    0.259181 \n",
       "  0.161415     0.347792    0.202863   -0.163116       -0.318149    -0.209124 \n",
       " -0.0144296   -0.201034   -0.222693   -0.0436117      -0.0129424   -0.021837 \n",
       "  0.0357635   -0.0196757   0.0162717  -0.0539516       0.0260143   -0.0321266\n",
       " -0.0709083   -0.0539753  -0.0971216   0.0266725   …   0.0084783    0.0754321\n",
       "  0.0412158   -0.0231214  -0.112432   -0.00943499     -0.0214947    0.0595807\n",
       "  0.00912263   0.166601    0.277468   -0.0251675      -0.127449     0.073971 \n",
       " -0.0925192   -0.123677   -0.261929   -0.059861        0.0381073    0.199565 \n",
       "  0.139945     0.12504     0.178481    0.143263        0.00231803  -0.226917 \n",
       " -0.470312    -0.500863   -0.642289   -0.0394096   …   0.411755     0.320678 \n",
       "  0.0989903    0.0706353   0.222132    0.16538         0.0440387   -0.310612 \n",
       " -0.128899    -0.0213592  -0.171049   -0.15382        -0.0580847    0.0846447\n",
       "  ⋮                                                ⋱                         \n",
       " -0.0148067   -0.240856   -0.298986   -0.0377453       0.0827832   -0.0142784\n",
       "  0.0884941    0.186744    0.0842403  -0.0582136      -0.123161    -0.081072 \n",
       " -0.040678    -0.129533   -0.023951    0.0966732   …   0.139441    -0.0405275\n",
       "  0.100297     0.254853    0.16825    -0.0364123      -0.169388    -0.0103332\n",
       " -0.0969496   -0.135442   -0.260817   -0.0480894       0.0112056    0.151857 \n",
       "  0.0767455    0.0412639   0.122786    0.0746696       0.021701    -0.149192 \n",
       "  0.0331939   -0.0398419   0.0183797   0.0275694       0.0235882   -0.0835265\n",
       " -0.00834324  -0.0849309  -0.327763    0.0151706   …   0.11336     -0.101438 \n",
       " -0.312246    -0.444065   -0.466251   -0.0233796       0.241362     0.345727 \n",
       "  0.356118     0.60691     0.767983   -0.0626957      -0.395983    -0.123244 \n",
       "  0.177907     0.279973    0.298825   -0.0185315      -0.174524    -0.108092 \n",
       "  0.298869     0.406116    0.596556    0.0168722      -0.324482    -0.287922 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(Float64, 1000, 1000)\n",
    "Ainv = inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the Sherman-Morrison formula!\n",
    "$$\\left( A+\\vec{u} \\vec{v}^T \\right)^{-1} = A^{-1} - \\frac{A^{-1} \\vec{u} \\vec{v}^TA^{-1}}{1 + \\vec{v}^T A^{-1} \\vec{u}}$$\n",
    "Of course, the vectors are vertical. I define $B = A+\\vec{u} \\vec{v}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       " 0.493476   0.287165  0.885528  0.281831  …  1.02257    1.00036   0.188449\n",
       " 0.53524    0.781394  0.236354  0.705672     0.276558   0.71931   0.722191\n",
       " 1.05341    0.15486   0.423554  0.836416     0.743585   0.290037  0.247521\n",
       " 0.954052   1.13119   1.51305   0.402359     1.07643    0.900526  0.567791\n",
       " 0.803435   0.314087  0.986208  0.941365     0.79413    0.622662  0.930937\n",
       " 0.422485   0.227933  0.75559   0.396224  …  0.800609   0.815683  1.05606 \n",
       " 1.03509    0.287621  1.12183   0.705295     0.714422   0.644318  0.216585\n",
       " 1.32684    0.735915  1.52469   0.454684     1.1374     1.11563   0.741174\n",
       " 0.373527   0.428652  0.938984  0.750711     0.367198   0.969151  0.130871\n",
       " 0.389354   0.660456  0.256996  0.728986     0.0664954  0.487686  0.53124 \n",
       " 0.748909   0.835971  0.528378  0.357133  …  1.26022    0.237799  0.594378\n",
       " 0.439319   0.602939  0.701492  0.871948     1.40638    0.685876  0.554511\n",
       " 0.578877   1.26819   0.348994  0.658416     0.570996   0.647767  0.744375\n",
       " ⋮                                        ⋱                               \n",
       " 0.657593   1.15048   0.856009  0.792497     0.414512   0.495477  0.76455 \n",
       " 0.916767   1.28249   0.579653  0.713933     1.06447    0.382201  1.016   \n",
       " 0.0717645  0.693339  0.973806  0.616103  …  0.258847   0.834     0.518462\n",
       " 0.849973   0.613556  0.590874  0.947582     0.872023   0.405787  0.884823\n",
       " 0.483477   0.948737  0.540857  0.567726     1.12542    0.201856  0.934525\n",
       " 0.590186   0.729635  0.822532  0.746454     0.921939   1.02275   0.623976\n",
       " 0.620905   0.985614  0.668349  1.06343      0.9607     0.715781  1.1286  \n",
       " 0.215774   0.644861  0.837573  0.314061  …  0.500586   1.1062    0.544463\n",
       " 0.179813   0.133204  0.514067  0.143629     0.487858   0.77255   0.511202\n",
       " 0.738628   0.372166  0.551532  0.120274     0.896147   0.771203  0.402893\n",
       " 0.835461   1.17977   0.364212  0.337742     0.5269     1.10779   0.468848\n",
       " 0.771964   0.8353    1.42189   0.980246     0.890063   0.353958  1.2473  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = rand(Float64, 1000)\n",
    "v = rand(Float64, 1000)\n",
    "B = copy(A)\n",
    "BLAS.ger!(1.0, u, v, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to copy the matrix first because most BLAS operations are destructive. The rank-1 update `BLAS.ger!` is a BLAS-2 function, so it is faster than the BLAS-3 function, matrix inversion. That's why we use the Sherman-Morrison formula to calculate $B^{-1}$ when we already know $A^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       " -0.210722    -0.165846    …  -0.344856    0.249913     0.170303 \n",
       " -0.108237    -0.0550062      -0.134173   -0.0451957    0.258381 \n",
       "  0.128561     0.256637        0.149242   -0.338014    -0.228196 \n",
       "  0.0532276   -0.0133194       0.214019    0.0279662    0.0174369\n",
       "  0.0342052   -0.023999       -0.0177607   0.0250721   -0.0330312\n",
       " -0.0227284    0.0796995   …   0.284146    0.03761      0.1034   \n",
       "  0.0398927   -0.0267921      -0.0550646  -0.0222947    0.0588127\n",
       " -0.0474696    0.00958601     -0.114276   -0.161667     0.0411201\n",
       " -0.0523626   -0.0122633      -0.0177558   0.0623877    0.222875 \n",
       "  0.132332     0.103917        0.143918   -0.00228513  -0.231336 \n",
       " -0.389001    -0.275268    …  -0.322295    0.460919     0.367877 \n",
       "  0.0987765    0.0700422       0.233175    0.0439095   -0.310736 \n",
       " -0.100418     0.0576604       0.11119    -0.040864     0.101177 \n",
       "  ⋮                        ⋱                                     \n",
       " -0.0256326   -0.270892       -0.449368    0.0762374   -0.0205627\n",
       "  0.103563     0.228553        0.391168   -0.114049    -0.0723246\n",
       " -0.0390067   -0.124896    …  -0.118055    0.140451    -0.0395574\n",
       "  0.0970119    0.245738        0.231997   -0.171374    -0.0122402\n",
       " -0.107828    -0.165624       -0.435069    0.00462818   0.145542 \n",
       "  0.0378082   -0.0667672      -0.203808   -0.00184222  -0.171795 \n",
       " -0.00370506  -0.142218       -0.27444     0.00127741  -0.104946 \n",
       "  0.0546973    0.0899745   …   0.206964    0.151477    -0.0648441\n",
       " -0.276236    -0.344154       -0.464455    0.263135     0.366631 \n",
       "  0.278004     0.390183        0.500115   -0.443215    -0.168588 \n",
       "  0.16482      0.243664        0.372477   -0.182437    -0.115688 \n",
       "  0.253274     0.279615        0.461801   -0.35205     -0.314389 "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Binv = copy(Ainv)\n",
    "BLAS.ger!(-1.0 / (1.0 + v' * Ainv * u), Ainv * u, (v' * Ainv)', Binv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7034651752394562e-10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(B * Binv - I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating rank-1 updates accumulates some error, so sometimes you have to refresh the updated matrix \"from scratch.\" (Be careful because sometimes it is not really from scratch.)\n",
    "\n",
    "As for LAPACK, most Julia functions on linear algebra are just wrappers of LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Array{Complex{Float64},1}:\n",
       "   499.64451884764486 + 0.0im                \n",
       "    7.326353262732669 + 5.467112951676198im  \n",
       "    7.326353262732669 - 5.467112951676198im  \n",
       "    4.511755204557462 + 8.083066699013186im  \n",
       "    4.511755204557462 - 8.083066699013186im  \n",
       "    9.028247612822707 + 1.1919623586751555im \n",
       "    9.028247612822707 - 1.1919623586751555im \n",
       "    7.821192282735549 + 4.4633601285490085im \n",
       "    7.821192282735549 - 4.4633601285490085im \n",
       "    6.479101407733715 + 6.1454582568154486im \n",
       "    6.479101407733715 - 6.1454582568154486im \n",
       "    6.669012289428684 + 5.778934893595768im  \n",
       "    6.669012289428684 - 5.778934893595768im  \n",
       "                      ⋮                      \n",
       " -0.21476999839163097 - 0.8102786144277376im \n",
       "   0.9245959366880042 + 0.0im                \n",
       "   0.9198014818236955 + 0.3774487000128153im \n",
       "   0.9198014818236955 - 0.3774487000128153im \n",
       "  0.09867037372449497 + 0.6285240450698899im \n",
       "  0.09867037372449497 - 0.6285240450698899im \n",
       "   0.6358888592001618 + 0.0im                \n",
       "  -0.4099507799568138 + 0.2032880630922363im \n",
       "  -0.4099507799568138 - 0.2032880630922363im \n",
       "  0.24122073526062746 + 0.14812071595954265im\n",
       "  0.24122073526062746 - 0.14812071595954265im\n",
       " -0.21021791574583268 + 0.0im                "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eigvals` is just a wrapper for `LAPACK.geevx!`, so you can directely call `LAPACK.geevx!` instead if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse matrix\n",
    "\n",
    "If your program is intensively using sparse matrices, you should use python instead because Julia only supports CSC matrix. Julia's native support for sparse matrices is not strong, so I do not recommend to write a code using multiple types of sparse matrices in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve a tight-binding model on the 2D square lattice in a poor man's way, i.e. in the real space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tuple{Tuple{Int64,Int64},Tuple{Int64,Int64}}[((1, 1), (2, 1)) ((1, 2), (2, 2)) … ((1, 29), (2, 29)) ((1, 30), (2, 30)); ((2, 1), (3, 1)) ((2, 2), (3, 2)) … ((2, 29), (3, 29)) ((2, 30), (3, 30)); … ; ((29, 1), (30, 1)) ((29, 2), (30, 2)) … ((29, 29), (30, 29)) ((29, 30), (30, 30)); ((30, 1), (1, 1)) ((30, 2), (1, 2)) … ((30, 29), (1, 29)) ((30, 30), (1, 30))], Tuple{Tuple{Int64,Int64},Tuple{Int64,Int64}}[((1, 1), (1, 2)) ((1, 2), (1, 3)) … ((1, 29), (1, 30)) ((1, 30), (1, 1)); ((2, 1), (2, 2)) ((2, 2), (2, 3)) … ((2, 29), (2, 30)) ((2, 30), (2, 1)); … ; ((29, 1), (29, 2)) ((29, 2), (29, 3)) … ((29, 29), (29, 30)) ((29, 30), (29, 1)); ((30, 1), (30, 2)) ((30, 2), (30, 3)) … ((30, 29), (30, 30)) ((30, 30), (30, 1))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const L = 30\n",
    "iter1D = 1 : L\n",
    "nnbondx = zip(Iterators.product(iter1D, iter1D), Iterators.product((mod1(i + 1, L) for i in iter1D), iter1D))\n",
    "nnbondy = zip(Iterators.product(iter1D, iter1D), Iterators.product(iter1D, (mod1(i + 1, L) for i in iter1D)))\n",
    "collect(nnbondx), collect(nnbondy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These iterators will generate the 2D square lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Base.Iterators.Zip2{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}}}},typeof(xytoz)}(xytoz, Base.Iterators.Zip2{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}}}}(Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}}((1:30, 1:30)), Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}}}((1:30, Base.Generator{UnitRange{Int64},getfield(Main, Symbol(\"##5#6\"))}(getfield(Main, Symbol(\"##5#6\"))(), 1:30)))))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xytoz(nn::Tuple{Tuple{Int64, Int64}, Tuple{Int64, Int64}}) = (nn[1][2] - 1) * L + nn[1][1], (nn[2][2] - 1) * L + nn[2][1]\n",
    "nnx = Base.Generator(xytoz, nnbondx)\n",
    "nny = Base.Generator(xytoz, nnbondy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Base.Generator(f, iter)` is same as `(f(x) for x in iter)`, or you can regard it as a lazy version of `map`, as you saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900×900 SparseMatrixCSC{Float64,Int64} with 3600 stored entries:\n",
       "  [2  ,   1]  =  -1.0\n",
       "  [30 ,   1]  =  -1.0\n",
       "  [31 ,   1]  =  -1.0\n",
       "  [871,   1]  =  -1.0\n",
       "  [1  ,   2]  =  -1.0\n",
       "  [3  ,   2]  =  -1.0\n",
       "  [32 ,   2]  =  -1.0\n",
       "  [872,   2]  =  -1.0\n",
       "  [2  ,   3]  =  -1.0\n",
       "  [4  ,   3]  =  -1.0\n",
       "  [33 ,   3]  =  -1.0\n",
       "  [873,   3]  =  -1.0\n",
       "  ⋮\n",
       "  [28 , 898]  =  -1.0\n",
       "  [868, 898]  =  -1.0\n",
       "  [897, 898]  =  -1.0\n",
       "  [899, 898]  =  -1.0\n",
       "  [29 , 899]  =  -1.0\n",
       "  [869, 899]  =  -1.0\n",
       "  [898, 899]  =  -1.0\n",
       "  [900, 899]  =  -1.0\n",
       "  [30 , 900]  =  -1.0\n",
       "  [870, 900]  =  -1.0\n",
       "  [871, 900]  =  -1.0\n",
       "  [899, 900]  =  -1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = L ^ 2\n",
    "H = spzeros(Float64, N, N)\n",
    "for (i, j) in Iterators.flatten((nnx, nny))\n",
    "    H[i, j] = -1.0\n",
    "    H[j, i] = -1.0\n",
    "end\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can rewrite this code by `zip`, but `zip` is not efficient here.\n",
    "\n",
    "Most of the operations for sparse matrices are similar to the ones for dense matrices. However, sparse arrays are more memory-efficient when the components of the matrix is almost zero. Especially, if the matrix is sparse enough, it significantly reduces the matrix muliplication cost from $O(N^3)$ to $O(N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000157 seconds (18 allocations: 268.500 KiB)\n",
      "  0.028663 seconds (6 allocations: 6.180 MiB)\n"
     ]
    }
   ],
   "source": [
    "Hdense = Array(H)\n",
    "H * H\n",
    "Hdense * Hdense\n",
    "@time H * H;\n",
    "@time Hdense * Hdense;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eigvals` does not support sparse matrices, so the calculation of the whole eigenvalues still costs $O(N^3)$. I will discuss this problem later in MCMC5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900-element Array{Float64,1}:\n",
       " -4.000000000000004 \n",
       " -3.9562952014676074\n",
       " -3.956295201467607 \n",
       " -3.9562952014676056\n",
       " -3.9562952014676043\n",
       " -3.9125904029352245\n",
       " -3.912590402935224 \n",
       " -3.9125904029352214\n",
       " -3.912590402935216 \n",
       " -3.827090915285207 \n",
       " -3.827090915285198 \n",
       " -3.8270909152851953\n",
       " -3.827090915285193 \n",
       "  ⋮                 \n",
       "  3.8270909152851997\n",
       "  3.8270909152852073\n",
       "  3.82709091528521  \n",
       "  3.9125904029352205\n",
       "  3.9125904029352228\n",
       "  3.912590402935223 \n",
       "  3.912590402935234 \n",
       "  3.9562952014676016\n",
       "  3.956295201467605 \n",
       "  3.956295201467612 \n",
       "  3.9562952014676163\n",
       "  3.999999999999999 "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(Hdense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the square (or cubic, etc.) lattice, you can directly begin from a dense matrix. Here's a smart implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900-element Array{Float64,1}:\n",
       " -4.000000000000004 \n",
       " -3.9562952014676074\n",
       " -3.956295201467607 \n",
       " -3.9562952014676056\n",
       " -3.9562952014676043\n",
       " -3.9125904029352245\n",
       " -3.912590402935224 \n",
       " -3.9125904029352214\n",
       " -3.912590402935216 \n",
       " -3.827090915285207 \n",
       " -3.827090915285198 \n",
       " -3.8270909152851953\n",
       " -3.827090915285193 \n",
       "  ⋮                 \n",
       "  3.8270909152851997\n",
       "  3.8270909152852073\n",
       "  3.82709091528521  \n",
       "  3.9125904029352205\n",
       "  3.9125904029352228\n",
       "  3.912590402935223 \n",
       "  3.912590402935234 \n",
       "  3.9562952014676016\n",
       "  3.956295201467605 \n",
       "  3.956295201467612 \n",
       "  3.9562952014676163\n",
       "  3.999999999999999 "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H4d = zeros(Float64, L, L, L, L)\n",
    "for ((i, j), (k, l)) in Iterators.flatten((nnbondx, nnbondy))\n",
    "    H4d[i, j, k, l] = -1.0\n",
    "    H4d[k, l, i, j] = -1.0\n",
    "end\n",
    "H2d = reshape(H4d, N, N)\n",
    "eigvals(H2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block checkerboard decomposition/approximation\n",
    "\n",
    "It is sometimes very useful to approximate a dense matrix by a product of sparse matrices. In the physical models like tight-binding models, block checkerboard docomposition will be a good approximation.\n",
    "\n",
    "~ under construction ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative solvers\n",
    "\n",
    "Iterative solvers, especially conjugate gradient methods are important for hybrid Monte Carlo simulations for lattice gauge theories.\n",
    "\n",
    "### Preconditioners\n",
    "\n",
    "FYI a careful choice of a preconditiner is necessary for ill-conditioned matrices, i.e. matrices with a large condition number. I personally recommend the incomplete Cholesky preconditioner in Preconditioners.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Preconditioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
