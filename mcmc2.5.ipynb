{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC2.5: Dense and Sparse Matrices\n",
    "\n",
    "This section is not necessarily for Marcov chain Monte Carlo itself, but the use of Sparse matrices is important to speed up quantum Monte Carlo simulations.\n",
    "\n",
    "## Dense matrix\n",
    "\n",
    "To use dense matrices, `LinearAlgebra` provides wrapper to BLAS/LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend Intel MKL instead of OpenBLAS. You can check whether MKL is used by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ":mkl"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BLAS.vendor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Array is called `Vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array{Int64, 1} == Vector{Int64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D Array is called `Matrix`, which I later call dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Array{Float64, 2} == Matrix{Float64}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Matrix` and `Array` of `Array` are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = [1 2\n",
    "                 3 4]\n",
    "array =[[1, 2], [3, 4]]\n",
    "matrix == array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia's support on 3-rank tensors is limited, but still we can define and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2×2 Array{Float64,3}:\n",
       "[:, :, 1] =\n",
       " 0.0  0.0\n",
       " 0.0  0.0\n",
       "\n",
       "[:, :, 2] =\n",
       " 0.0  0.0\n",
       " 0.0  0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = zeros(Float64, 2, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, matrices are stored columnwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000035 seconds (3 allocations: 78.219 KiB)\n",
      "  0.000309 seconds (3 allocations: 78.219 KiB)\n"
     ]
    }
   ],
   "source": [
    "mat = ones(10000, 10000)\n",
    "mat[:, 1]\n",
    "mat[1, :]\n",
    "@time mat[:, 1];\n",
    "@time mat[1, :];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, vertical vectors are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 17.0\n",
       " 39.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1.0 2.0; 3.0 4.0] * [5.0; 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLAS/LAPACK\n",
    "\n",
    "Dense matrices are not memory-efficient, but support BLAS/LAPACK. The most important BLAS operation in quantum Monte Carlo is rank-1 update (or other finite-rank updates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       "  13.5039    15.3539   -3.48992  …  -2.73049    12.3363     29.0417\n",
       "   7.62503    8.80477  -1.87871     -1.55453     7.05808    16.5354\n",
       " -19.3761   -22.0313    5.02901      3.96615   -17.7504    -41.662\n",
       "  -9.67815  -11.0603    2.45262      2.05864    -8.94929   -20.9152\n",
       "   4.43783    4.99657  -1.27742     -0.983852    4.09454     9.68627\n",
       " -43.1138   -49.3089   10.8197   …   8.75823   -39.6746    -92.8666\n",
       "   7.94839    9.01022  -2.28172     -1.63392     7.17622    17.212\n",
       " -30.9874   -35.3507    7.8061       6.30045   -28.459     -66.6375\n",
       " -11.6841   -13.3722    3.04768      2.30612   -10.728     -25.156\n",
       "  11.9116    13.6195   -3.04198     -2.36742    10.9239     25.7039\n",
       " -52.1984   -59.7845   13.1481   …  10.536     -48.0931   -112.588\n",
       " -19.9745   -22.7966    4.93357      4.10266   -18.4044    -42.9596\n",
       " -13.4706   -15.2518    3.30552      2.7037    -12.3842    -28.7587\n",
       "   ⋮                             ⋱                        \n",
       "  29.971     34.1592   -7.85665     -6.0787     27.3682     64.664\n",
       "  -7.59491   -8.57977   1.96673      1.48611    -7.03211   -16.32\n",
       "  -8.88823  -10.2268    2.28308  …   1.81437    -8.2344    -19.3207\n",
       " -21.3408   -24.4826    5.24758      4.29915   -19.5533    -45.9165\n",
       " -30.8521   -35.2616    7.77772      6.25848   -28.4817    -66.4949\n",
       " -11.1278   -12.8924    2.81341      2.23068   -10.2553    -24.0412\n",
       " -10.165    -11.6748    2.45286      2.12859    -9.4074    -22.0148\n",
       " -26.6113   -30.6767    6.45396  …   5.42981   -24.5584    -57.3961\n",
       "  -4.64991   -5.35911   1.0651       1.02861    -4.3229    -10.0108\n",
       "  -9.5477   -10.9079    2.45997      1.85183    -8.76194   -20.5641\n",
       " -13.0028   -14.8313    3.33796      2.67498   -11.9915    -28.1033\n",
       " -44.244    -50.7384   11.1716       9.03273   -40.5464    -95.3648"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = rand(Float64, 1000, 1000)\n",
    "Ainv = inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the Sherman-Morrison formula!\n",
    "$$\\left( A+\\vec{u} \\vec{v}^T \\right)^{-1} = A^{-1} - \\frac{A^{-1} \\vec{u} \\vec{v}^TA^{-1}}{1 + \\vec{v}^T A^{-1} \\vec{u}}$$\n",
    "Of course, the vectors are vertical. I define $B = A+\\vec{u} \\vec{v}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       " 1.31962   1.11814   0.504182   1.00876   …  1.02839   0.304281  0.768145\n",
       " 0.871257  0.386744  1.32125    0.598344     0.914128  0.192313  0.864541\n",
       " 0.922351  0.693441  1.00169    0.546869     0.657073  0.68525   0.882303\n",
       " 0.765574  0.545746  0.710324   0.875794     1.49538   0.327563  0.371543\n",
       " 0.742203  0.209361  1.00277    0.348439     0.541256  0.411337  0.965191\n",
       " 0.790789  0.554229  1.21141    1.1647    …  1.14102   0.504251  0.571595\n",
       " 0.957804  0.762132  1.1768     0.873347     0.855631  0.565383  0.336774\n",
       " 1.25337   0.731997  0.957876   1.24763      1.60365   0.427604  0.523079\n",
       " 1.20456   1.15702   0.744122   0.991615     1.59652   0.786371  0.616771\n",
       " 1.42059   0.416556  1.28344    0.912905     1.41753   0.667684  0.396756\n",
       " 1.37267   0.686352  1.49119    0.29229   …  1.66387   0.841703  1.10397\n",
       " 0.602746  0.347378  0.0842355  0.711546     0.776959  0.953345  0.927194\n",
       " 0.961847  1.15944   0.684711   0.542454     1.15371   0.883437  1.07637\n",
       " ⋮                                        ⋱                      \n",
       " 0.905395  0.711987  0.716406   0.398673     1.31902   0.64261   0.558051\n",
       " 1.5443    0.850415  0.741546   1.04836      0.779808  0.554521  0.414486\n",
       " 1.05277   0.833337  0.684202   0.282271  …  0.614995  0.9146    0.863138\n",
       " 0.405956  0.326673  0.988985   0.63884      0.956238  0.587816  0.740606\n",
       " 0.85455   1.12289   0.414413   0.690163     0.68624   0.176893  0.356501\n",
       " 0.883938  1.21983   1.15041    0.733359     1.56262   0.875213  0.804735\n",
       " 1.24136   0.586117  1.07114    0.654563     1.17416   0.214585  0.229805\n",
       " 1.0632    0.922958  0.86066    0.424658  …  1.09898   0.459669  0.724301\n",
       " 0.75777   0.608281  0.613633   0.409256     0.267069  0.998158  0.22362\n",
       " 1.24307   0.844631  1.4254     0.560005     1.40192   0.742031  0.921241\n",
       " 0.18581   0.538643  1.02502    0.688411     0.509811  0.181414  0.213461\n",
       " 0.899709  1.0011    0.389957   0.911193     0.912373  0.101931  0.189973"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = rand(Float64, 1000)\n",
    "v = rand(Float64, 1000)\n",
    "B = copy(A)\n",
    "BLAS.ger!(1.0, u, v, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to copy the matrix first because most BLAS operations are destructive. The rank-1 update `BLAS.ger!` is a BLAS-2 function, so it is faster than the BLAS-3 function, matrix inversion. That's why we use the Sherman-Morrison formula to calculate $B^{-1}$ when we already know $A^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×1000 Array{Float64,2}:\n",
       "  0.119966    -0.0163774   -0.21392    …  -0.0230391   0.172599\n",
       "  0.0437802    0.0983464   -0.0230376      0.0571958   0.182643\n",
       " -0.288823    -0.111184     0.356999      -0.124362   -0.490794\n",
       " -0.038557     0.00991579   0.0931188     -0.0476346  -0.122525\n",
       "  0.100855     0.0159234   -0.21585        0.0895744   0.331404\n",
       " -0.124445     0.0607231    0.297105   …   0.023855   -0.138314\n",
       "  0.219782     0.134564    -0.389976       0.0392566   0.541377\n",
       " -0.0631598    0.16318      0.236729       0.0978763   0.0661498\n",
       " -0.128064    -0.101056     0.219092      -0.0566283  -0.229555\n",
       " -0.00690611  -0.0678896   -0.124658      -0.0822608  -0.00441265\n",
       " -0.220018    -0.0917433    0.425298   …  -0.0937677  -0.470078\n",
       " -0.141938    -0.0205977    0.0791179     -0.0900893  -0.180678\n",
       " -0.156635     0.0381804    0.046644      -0.08948    -0.0404019\n",
       "  ⋮                                    ⋱              \n",
       "  0.217401    -0.0103242   -0.57381       -0.107767    0.4853\n",
       " -0.23254     -0.124711     0.164636      -0.233342   -0.43936\n",
       "  0.0911019    0.0852132    0.0851906  …   0.0575507   0.0477975\n",
       " -0.002998     0.0220454    0.0247073      0.151001    0.10915\n",
       " -0.145307     0.00255332   0.261563      -0.12555    -0.260169\n",
       " -0.0327928   -0.15071      0.0976649     -0.009619   -0.109191\n",
       "  0.0923746    0.104981    -0.0578562      0.0647676   0.110484\n",
       "  0.0558432   -0.0517179   -0.0733921  …   0.0673054   0.125067\n",
       "  0.0441599    0.0316284   -0.0838764      0.0118288   0.114372\n",
       " -0.0753741   -0.0296962    0.141413      -0.0147459  -0.132286\n",
       " -0.0538739    0.0394489    0.168437      -0.0338143  -0.172442\n",
       " -0.126342    -0.0730432    0.372869       0.193955   -0.202868"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Binv = copy(Ainv)\n",
    "BLAS.ger!(-1.0 / (1.0 + v' * Ainv * u), Ainv * u, (v' * Ainv)', Binv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4955690177227296e-8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(B * Binv - I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating rank-1 updates accumulates some error, so sometimes you have to refresh the updated matrix \"from scratch.\" (Be careful because sometimes it is not really from scratch.)\n",
    "\n",
    "As for LAPACK, most Julia functions on linear algebra are just wrappers of LAPACK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Array{Complex{Float64},1}:\n",
       "  -9.17963412449858 - 0.3967747294588133im\n",
       "  -9.17963412449858 + 0.3967747294588133im\n",
       " -8.884699578737584 - 1.5226368949651186im\n",
       " -8.884699578737584 + 1.5226368949651186im\n",
       " -8.821210665710549 - 2.549086755691488im\n",
       " -8.821210665710549 + 2.549086755691488im\n",
       "  -8.59176678823733 - 1.244614505677139im\n",
       "  -8.59176678823733 + 1.244614505677139im\n",
       " -8.562123436638599 - 0.7830136033436652im\n",
       " -8.562123436638599 + 0.7830136033436652im\n",
       " -8.489307268583463 - 3.427655771340261im\n",
       " -8.489307268583463 + 3.427655771340261im\n",
       "  -8.48613583604433 - 0.2064407932532244im\n",
       "                    ⋮\n",
       "  8.587061658799243 - 0.685493300392522im\n",
       "  8.587061658799243 + 0.685493300392522im\n",
       "  8.747464302084035 - 2.800026342468835im\n",
       "  8.747464302084035 + 2.800026342468835im\n",
       "  8.824483310762423 - 2.2780877172214957im\n",
       "  8.824483310762423 + 2.2780877172214957im\n",
       "  8.867285298499286 - 1.4480387633889629im\n",
       "  8.867285298499286 + 1.4480387633889629im\n",
       "  8.932823301103733 + 0.0im\n",
       "  9.130148332639923 - 0.3373681456779261im\n",
       "  9.130148332639923 + 0.3373681456779261im\n",
       "  500.0583484570641 + 0.0im"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eigvals` is just a wrapper for `LAPACK.geevx!`, so you can directely call `LAPACK.geevx!` instead if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse matrix\n",
    "\n",
    "If your program is intensively using sparse matrices, you should use python instead because Julia only supports CSC matrix. Julia's native support for sparse matrices is not strong, so I do not recommend to write a code using multiple types of sparse matrices in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SparseArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's solve a tight-binding model on the 2D square lattice in a poor man's way, i.e. in the real space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([((1, 1), (2, 1)) ((1, 2), (2, 2)) … ((1, 29), (2, 29)) ((1, 30), (2, 30)); ((2, 1), (3, 1)) ((2, 2), (3, 2)) … ((2, 29), (3, 29)) ((2, 30), (3, 30)); … ; ((29, 1), (30, 1)) ((29, 2), (30, 2)) … ((29, 29), (30, 29)) ((29, 30), (30, 30)); ((30, 1), (1, 1)) ((30, 2), (1, 2)) … ((30, 29), (1, 29)) ((30, 30), (1, 30))], [((1, 1), (1, 2)) ((1, 2), (1, 3)) … ((1, 29), (1, 30)) ((1, 30), (1, 1)); ((2, 1), (2, 2)) ((2, 2), (2, 3)) … ((2, 29), (2, 30)) ((2, 30), (2, 1)); … ; ((29, 1), (29, 2)) ((29, 2), (29, 3)) … ((29, 29), (29, 30)) ((29, 30), (29, 1)); ((30, 1), (30, 2)) ((30, 2), (30, 3)) … ((30, 29), (30, 30)) ((30, 30), (30, 1))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const L = 30\n",
    "iter1D = 1 : L\n",
    "nnbondx = zip(Iterators.product(iter1D, iter1D), Iterators.product((mod1(i + 1, L) for i in iter1D), iter1D))\n",
    "nnbondy = zip(Iterators.product(iter1D, iter1D), Iterators.product(iter1D, (mod1(i + 1, L) for i in iter1D)))\n",
    "collect(nnbondx), collect(nnbondy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These iterators will generate the 2D square lattice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Base.Generator{Base.Iterators.Zip{Tuple{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},var\"#5#6\"}}}}},typeof(xytoz)}(xytoz, Base.Iterators.Zip{Tuple{Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}},Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},var\"#5#6\"}}}}}((Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},UnitRange{Int64}}}((1:30, 1:30)), Base.Iterators.ProductIterator{Tuple{UnitRange{Int64},Base.Generator{UnitRange{Int64},var\"#5#6\"}}}((1:30, Base.Generator{UnitRange{Int64},var\"#5#6\"}(var\"#5#6\"(), 1:30))))))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xytoz(nn::Tuple{Tuple{Int64, Int64}, Tuple{Int64, Int64}}) = (nn[1][2] - 1) * L + nn[1][1], (nn[2][2] - 1) * L + nn[2][1]\n",
    "nnx = Base.Generator(xytoz, nnbondx)\n",
    "nny = Base.Generator(xytoz, nnbondy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Base.Generator(f, iter)` is same as `(f(x) for x in iter)`, or you can regard it as a lazy version of `map`, as you saw before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900×900 SparseMatrixCSC{Float64,Int64} with 3600 stored entries:\n",
       "  [2  ,   1]  =  -1.0\n",
       "  [30 ,   1]  =  -1.0\n",
       "  [31 ,   1]  =  -1.0\n",
       "  [871,   1]  =  -1.0\n",
       "  [1  ,   2]  =  -1.0\n",
       "  [3  ,   2]  =  -1.0\n",
       "  [32 ,   2]  =  -1.0\n",
       "  [872,   2]  =  -1.0\n",
       "  [2  ,   3]  =  -1.0\n",
       "  [4  ,   3]  =  -1.0\n",
       "  [33 ,   3]  =  -1.0\n",
       "  [873,   3]  =  -1.0\n",
       "  ⋮\n",
       "  [898, 897]  =  -1.0\n",
       "  [28 , 898]  =  -1.0\n",
       "  [868, 898]  =  -1.0\n",
       "  [897, 898]  =  -1.0\n",
       "  [899, 898]  =  -1.0\n",
       "  [29 , 899]  =  -1.0\n",
       "  [869, 899]  =  -1.0\n",
       "  [898, 899]  =  -1.0\n",
       "  [900, 899]  =  -1.0\n",
       "  [30 , 900]  =  -1.0\n",
       "  [870, 900]  =  -1.0\n",
       "  [871, 900]  =  -1.0\n",
       "  [899, 900]  =  -1.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = L ^ 2\n",
    "H = spzeros(Float64, N, N)\n",
    "for (i, j) in Iterators.flatten((nnx, nny))\n",
    "    H[i, j] = -1.0\n",
    "    H[j, i] = -1.0\n",
    "end\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can rewrite this code by `zip`, but `zip` is not efficient here.\n",
    "\n",
    "Most of the operations for sparse matrices are similar to the ones for dense matrices. However, sparse arrays are more memory-efficient when the components of the matrix is almost zero. Especially, if the matrix is sparse enough, it significantly reduces the matrix muliplication cost from $O(N^3)$ to $O(N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000179 seconds (8 allocations: 253.766 KiB)\n",
      "  0.018984 seconds (2 allocations: 6.180 MiB)\n"
     ]
    }
   ],
   "source": [
    "Hdense = Array(H)\n",
    "H * H\n",
    "Hdense * Hdense\n",
    "@time H * H;\n",
    "@time Hdense * Hdense;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eigvals` does not support sparse matrices, so the calculation of the whole eigenvalues still costs $O(N^3)$. I will discuss this problem later in MCMC5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900-element Array{Float64,1}:\n",
       " -4.000000000000008\n",
       " -3.9562952014676234\n",
       " -3.9562952014676207\n",
       " -3.9562952014676123\n",
       " -3.956295201467606\n",
       " -3.912590402935227\n",
       " -3.912590402935225\n",
       " -3.9125904029352196\n",
       " -3.912590402935214\n",
       " -3.8270909152852037\n",
       " -3.827090915285201\n",
       " -3.8270909152852006\n",
       " -3.827090915285196\n",
       "  ⋮\n",
       "  3.8270909152852006\n",
       "  3.8270909152852024\n",
       "  3.827090915285208\n",
       "  3.9125904029352183\n",
       "  3.912590402935222\n",
       "  3.9125904029352276\n",
       "  3.9125904029352276\n",
       "  3.9562952014676083\n",
       "  3.956295201467612\n",
       "  3.9562952014676123\n",
       "  3.9562952014676154\n",
       "  4.000000000000006"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigvals(Hdense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the square (or cubic, etc.) lattice, you can directly begin from a dense matrix. Here's a smart implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900-element Array{Float64,1}:\n",
       " -4.000000000000008\n",
       " -3.9562952014676234\n",
       " -3.9562952014676207\n",
       " -3.9562952014676123\n",
       " -3.956295201467606\n",
       " -3.912590402935227\n",
       " -3.912590402935225\n",
       " -3.9125904029352196\n",
       " -3.912590402935214\n",
       " -3.8270909152852037\n",
       " -3.827090915285201\n",
       " -3.8270909152852006\n",
       " -3.827090915285196\n",
       "  ⋮\n",
       "  3.8270909152852006\n",
       "  3.8270909152852024\n",
       "  3.827090915285208\n",
       "  3.9125904029352183\n",
       "  3.912590402935222\n",
       "  3.9125904029352276\n",
       "  3.9125904029352276\n",
       "  3.9562952014676083\n",
       "  3.956295201467612\n",
       "  3.9562952014676123\n",
       "  3.9562952014676154\n",
       "  4.000000000000006"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H4d = zeros(Float64, L, L, L, L)\n",
    "for ((i, j), (k, l)) in Iterators.flatten((nnbondx, nnbondy))\n",
    "    H4d[i, j, k, l] = -1.0\n",
    "    H4d[k, l, i, j] = -1.0\n",
    "end\n",
    "H2d = reshape(H4d, N, N)\n",
    "eigvals(H2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block checkerboard decomposition/approximation\n",
    "\n",
    "It is sometimes very useful to approximate a dense matrix by a product of sparse matrices. In the physical models like tight-binding models, block checkerboard docomposition will be a good approximation.\n",
    "\n",
    "~ under construction ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative solvers\n",
    "\n",
    "Iterative solvers, especially conjugate gradient methods are important for hybrid Monte Carlo simulations for lattice gauge theories.\n",
    "\n",
    "### Conjugate gradient (CG) method\n",
    "\n",
    "~ under construction ~\n",
    "\n",
    "### Preconditioners\n",
    "\n",
    "FYI a careful choice of a preconditiner is necessary for ill-conditioned matrices, i.e. matrices with a large condition number. I personally recommend the incomplete Cholesky preconditioner in Preconditioners.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
