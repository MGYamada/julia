{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC3.5: Jackknife Resampling\n",
    "\n",
    "To estimate the error of observables accurately, it is recommended to use the Jackknife resampling method.\n",
    "\n",
    "## Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementflux (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ResumableFunctions\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "const Jx = 1 / 3 # oppposite sign to Motome's\n",
    "const Jy = 1 / 3\n",
    "const Jz = 1 / 3\n",
    "\"\"\"\n",
    "Metropolis method\n",
    "\"\"\"\n",
    "function Metropolis(βF::Float64, βFnew::Float64)::Bool\n",
    "    βF - βFnew > log(rand())\n",
    "end\n",
    "\"\"\"\n",
    "Generating a honeycomb lattice with an open boundary condition.\n",
    "\"\"\"\n",
    "function openhoneycomb(Lx::Int64, Ly::Int64)::Tuple\n",
    "    N = 2Lx * Ly\n",
    "    nnx = zip(1 : 2 : (N - 1), 2 : 2 : N)\n",
    "    nny = Iterators.flatten((zip((1 + 2i) : 2Lx : (2Lx * (Ly - 1)  + 1 + 2i), 2i : 2Lx : (2Lx * (Ly - 1)  + 2i)) for i in 1 : (Lx - 1)))\n",
    "    nnz = zip(1 : 2 : (N - 1), Iterators.flatten(((2Lx + 2) : 2 : N, 2 : 2 : 2Lx)))\n",
    "    plaquette = Iterators.flatten(zip((Lx * (i - 1) + 1) : (Lx * (i - 1) + Lx - 1), (Lx * (i - 1) + 2) : (Lx * (i - 1) + Lx)) for i in 1 : Ly)\n",
    "    N, nnx, nny, nnz, plaquette\n",
    "end\n",
    "@resumable function measurementflux(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Float64\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 2.0im * J\n",
    "        h[nn[2], nn[1]] = -2.0im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0\n",
    "    hdense = Array(h)\n",
    "    plaq = collect(plaquette)\n",
    "    Np = length(plaq)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            ev = eigvals(Hermitian(hdense))\n",
    "            positiveev = Iterators.drop(ev, N >> 1)\n",
    "            βFnew = -sum((log(2.0 * cosh(β * ϵ / 2.0)) for ϵ in positiveev))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        @yield sum((η[k] * η[l] for (k, l) in plaq)) / Np\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason of the error bar is the discreteness of the returned value `flux`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "-0.16666666666666666\n",
      "0.6666666666666666\n",
      "0.0\n",
      "0.6666666666666666\n",
      "0.6666666666666666\n",
      "0.16666666666666666\n",
      "0.3333333333333333\n",
      "0.6666666666666666\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "mcstep = Iterators.drop(measurementflux(Metropolis, openhoneycomb, 25.0, 4, 4), 10000)\n",
    "foreach(println, Iterators.take(mcstep, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to flatten these quantized values by binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.2941666666666667 \n",
       " 0.27316666666666667\n",
       " 0.2955             \n",
       " 0.2854999999999999 \n",
       " 0.3018333333333334 \n",
       " 0.307              \n",
       " 0.2776666666666667 \n",
       " 0.29766666666666663\n",
       " 0.30616666666666664\n",
       " 0.2734999999999999 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics\n",
    "Nsample = 10000\n",
    "Nbin = 10\n",
    "Nbinsize = Nsample ÷ Nbin\n",
    "iter = Iterators.partition(Iterators.take(mcstep, Nsample), Nbinsize)\n",
    "bin = collect(map(mean, iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2912166666666666 ± 0.0040959969919452735\n"
     ]
    }
   ],
   "source": [
    "m = mean(bin)\n",
    "s = stdm(bin, m) / sqrt(length(bin))\n",
    "println(\"$m ± $s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Nbinsize` has to be determined based on the autocorrelation. In order to reduce `Nbin` to get a more acculate result at low temperature, we need to implement some global updating algorithm.\n",
    "\n",
    "## Delete-1 jackknife resampling\n",
    "\n",
    "Delete-1 jackknife resampling is simply implemented in https://github.com/ararslan/Jackknife.jl. However, the function is limited, so I will newly define functions for the jackknife resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdJ"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Delete one sample from given samples. before and after are applied before and after taking an expectation value.\n",
    "\"\"\"\n",
    "function leaveoneout(before::Function, after::Function, v::AbstractVector)\n",
    "    ind = eachindex(v)\n",
    "    map(i -> after(mean(map(before, view(v, filter(!isequal(i), ind))))), ind)\n",
    "end\n",
    "\"\"\"\n",
    "Calculate an expectation value by jackknife.\n",
    "\"\"\"\n",
    "meanJ(b::Function, a::Function, v::AbstractVector) = mean(leaveoneout(b, a, v))\n",
    "\"\"\"\n",
    "Calculate an error bar with given mean by jackknife.\n",
    "\"\"\"\n",
    "stdmJ(b::Function, a::Function, v::AbstractVector, m) = stdm(leaveoneout(b, a, v), m, corrected = false) * sqrt(length(v) - 1)\n",
    "\"\"\"\n",
    "Calculate an error bar by jackknife.\n",
    "\"\"\"\n",
    "stdJ(b::Function, a::Function, v::AbstractVector) = stdmJ(b, a, v, meanJ(b, a, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions are based on Statistics.jl and Jackknife.jl, so please see their reference to know how it works. I again use `measurementEf` as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementEf (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@resumable function measurementEf(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Vector{Float64}\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 2.0im * J\n",
    "        h[nn[2], nn[1]] = -2.0im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0\n",
    "    β₂ = β * 0.5\n",
    "    hdense = Array(h)\n",
    "    ev = zeros(Float64, N)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            evnew = eigvals(Hermitian(hdense))\n",
    "            βFnew = -sum(@. log(exp(β₂ * evnew[(N >> 1 + 1) : end]) + exp(-β₂ * evnew[(N >> 1 + 1) : end])))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "                ev .= evnew\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        Ef = -sum(@. ev[(N >> 1 + 1) : end] * tanh(β₂ * ev[(N >> 1 + 1) : end] )) * 0.5\n",
    "        ∂Ef∂β = -sum(@. (ev[(N >> 1 + 1) : end] ^ 2) * (sech(β₂ * ev[(N >> 1 + 1) : end]) ^ 2)) * 0.25\n",
    "        @yield [Ef, ∂Ef∂β]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ok to first assume the bin size to be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Array{Float64,1},1}:\n",
       " [-7.65199, -0.00681468]\n",
       " [-7.71633, -0.0109046] \n",
       " [-7.6112, -0.00884734] \n",
       " [-7.62375, -0.0140109] \n",
       " [-7.66819, -0.00665184]\n",
       " [-7.72563, -0.0118945] \n",
       " [-7.69165, -0.0119024] \n",
       " [-7.72551, -0.0172385] \n",
       " [-7.7481, -0.0151541]  \n",
       " [-7.61735, -0.00808208]\n",
       " [-7.72379, -0.0172919] \n",
       " [-7.64852, -0.012708]  \n",
       " [-7.76376, -0.0145314] \n",
       " ⋮                      \n",
       " [-7.65265, -0.00731072]\n",
       " [-7.63267, -0.0111899] \n",
       " [-7.70305, -0.0115475] \n",
       " [-7.74876, -0.010281]  \n",
       " [-7.59937, -0.00871572]\n",
       " [-7.72178, -0.011896]  \n",
       " [-7.66303, -0.00911828]\n",
       " [-7.67054, -0.00882789]\n",
       " [-7.75, -0.0154435]    \n",
       " [-7.70757, -0.0053772] \n",
       " [-7.70305, -0.0115475] \n",
       " [-7.70387, -0.0108522] "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const β = 10.0\n",
    "mcstep2 = Iterators.drop(measurementEf(Metropolis, openhoneycomb, β, 4, 4), 10000)\n",
    "iter2 = Iterators.take(mcstep2, Nsample)\n",
    "data = collect(iter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `after` = `before` = `identity`, `meanJ` and `stdmJ `work in the same way as `mean` and `stdm`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ef = -7.69119858657557 ± 0.000666412231282982, ∂Ef∂β = -0.010792833024182637 ± 2.615725444754717e-5\n"
     ]
    }
   ],
   "source": [
    "m2 = meanJ(identity, identity, data)\n",
    "s2 = stdmJ(identity, identity, data, m2)\n",
    "println(\"Ef = $(m2[1]) ± $(s2[1]), ∂Ef∂β = $(m2[2]) ± $(s2[2])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agrees with the standard estimation method for the error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.0006664122312814925\n",
       " 2.615725444754927e-5 "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std(data) / sqrt(length(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such mean values, the jackknife resampling is apparently overkill. However, to estimate the error for the values like the specific heat, the jackknife resampling is very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cv = 1.5233441494605942 ± 0.006114923879971831\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function applied before mean to calculate heat capacity.\n",
    "\"\"\"\n",
    "TTCv(v::Vector{Float64}) = [v[1] ^ 2 - v[2], v[1]]\n",
    "\"\"\"\n",
    "Function applied after mean to calculate heat capacity.\n",
    "\"\"\"\n",
    "function Cv(β::Float64)::Function\n",
    "    β² = β ^ 2\n",
    "    meanTTCv::Vector{Float64} -> β² * (meanTTCv[1] - meanTTCv[2] ^ 2)\n",
    "end\n",
    "m3 = meanJ(TTCv, Cv(β), data)\n",
    "s3 = stdmJ(TTCv, Cv(β), data, m3)\n",
    "println(\"Cv = $m3 ± $s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "The simplest way to estimate autocorrelation is by changing the size of binning and estimating its errors by jackknife resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000-element Array{Array{Float64,1},1}:\n",
       " [59.0562, -7.68416]\n",
       " [58.0374, -7.61747]\n",
       " [59.2525, -7.69691]\n",
       " [59.4371, -7.70858]\n",
       " [59.0402, -7.68273]\n",
       " [59.0934, -7.68615]\n",
       " [60.3524, -7.76779]\n",
       " [58.6589, -7.65778]\n",
       " [58.7264, -7.66285]\n",
       " [60.1119, -7.75262]\n",
       " [60.1304, -7.75363]\n",
       " [58.4184, -7.64244]\n",
       " [59.6763, -7.72421]\n",
       " ⋮                  \n",
       " [59.3942, -7.70598]\n",
       " [57.9105, -7.60882]\n",
       " [59.6925, -7.72535]\n",
       " [59.5882, -7.71856]\n",
       " [59.8915, -7.73805]\n",
       " [59.9906, -7.74449]\n",
       " [58.4196, -7.64266]\n",
       " [59.7011, -7.72591]\n",
       " [58.6985, -7.66058]\n",
       " [58.7886, -7.66678]\n",
       " [59.7449, -7.72878]\n",
       " [59.3545, -7.70346]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binning = Iterators.partition(data, 2)\n",
    "bin2 = mean.(TTCv, binning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cv = 1.523344144830859 ± 0.006104176605477246\n"
     ]
    }
   ],
   "source": [
    "m4 = meanJ(identity, Cv(β), bin2)\n",
    "s4 = stdmJ(identity, Cv(β), bin2, m4)\n",
    "println(\"Cv = $m4 ± $s4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the binsize does not affect the expectation value (or the errorbar) too much means that the autocorrelation length is about 1 step. Note that at low temperature the binsize strongly affects the expectation value, which means that the binsize must be taken to be large enough. Here I estimate the autocorrelation length by another method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41-element Array{Float64,1}:\n",
       "  1.0                  \n",
       "  0.025491435519586603 \n",
       " -0.006579779196292195 \n",
       "  0.01972258574897958  \n",
       "  0.009057397719596889 \n",
       "  0.012365335991191074 \n",
       " -0.002196061267901087 \n",
       "  0.0007310655723478059\n",
       " -0.026119823427892427 \n",
       "  0.0006152457871164597\n",
       "  0.013781980507166112 \n",
       "  0.008433945828489973 \n",
       "  0.01660415867547176  \n",
       "  ⋮                    \n",
       " -0.004763807913707725 \n",
       "  0.015589687368502034 \n",
       "  0.010084632767619416 \n",
       " -0.006740418152846602 \n",
       "  0.012665278597385149 \n",
       " -0.004961264798734328 \n",
       "  0.014650798162212458 \n",
       "  0.003923249971725114 \n",
       " -0.008457426685980993 \n",
       "  0.01704902845265379  \n",
       "  0.01526012826675791  \n",
       " -0.014536009967628142 "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsBase\n",
    "StatsBase.autocor(first.(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rapid decay in the autocorrelation function means that the autocorrelation length is less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap method\n",
    "\n",
    "~ under construction ~\n",
    "\n",
    "**Exercise**: implement a Bootstrap method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
