{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC3.5: Jackknife Resampling\n",
    "\n",
    "To estimate the error of observables accurately, it is recommended to use the Jackknife resampling method.\n",
    "\n",
    "## Binning\n",
    "\n",
    "Previously, we faced a large errorbar problem at low temperature in this program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementflux (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ResumableFunctions\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "const Jx = 1 / 3 # oppposite sign to Motome's\n",
    "const Jy = 1 / 3\n",
    "const Jz = 1 / 3\n",
    "function Metropolis(βF::Float64, βFnew::Float64)::Bool\n",
    "    βF - βFnew > log(rand())\n",
    "end\n",
    "function openhoneycomb(Lx::Int64, Ly::Int64)::Tuple\n",
    "    N = 2Lx * Ly\n",
    "    nnx = zip(1 : 2 : (N - 1), 2 : 2 : N)\n",
    "    nny = Iterators.flatten(zip((1 + 2i) : 2Lx : (2Lx * (Ly - 1)  + 1 + 2i), 2i : 2Lx : (2Lx * (Ly - 1)  + 2i)) for i in 1 : (Lx - 1))\n",
    "    nnz = zip(1 : 2 : (N - 1), Iterators.flatten(((2Lx + 2) : 2 : N, 2 : 2 : 2Lx)))\n",
    "    plaquette = Iterators.flatten(zip((Lx * (i - 1) + 1) : (Lx * (i - 1) + Lx - 1), (Lx * (i - 1) + 2) : (Lx * (i - 1) + Lx)) for i in 1 : Ly)\n",
    "    N, nnx, nny, nnz, plaquette\n",
    "end\n",
    "@resumable function measurementflux(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Float64\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 0.5im * J\n",
    "        h[nn[2], nn[1]] = -0.5im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0\n",
    "    hdense = Array(h)\n",
    "    plaq = collect(plaquette)\n",
    "    Np = length(plaq)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            ev = eigvals(Hermitian(hdense))\n",
    "            positiveev = Iterators.drop(ev, N >> 1)\n",
    "            βFnew = -sum((log(2.0 * cosh(β * ϵ / 2.0)) for ϵ in positiveev))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        @yield sum((η[k] * η[l] for (k, l) in plaq)) / Np\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason is the discreteness of the returned value \"flux.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16666666666666666\n",
      "0.16666666666666666\n",
      "0.5\n",
      "0.6666666666666666\n",
      "0.6666666666666666\n",
      "0.8333333333333334\n",
      "0.5\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "mcstep = Iterators.drop(measurementflux(Metropolis, openhoneycomb, 100.0, 4, 4), 10000)\n",
    "foreach(println, Iterators.take(mcstep, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to flatten these quantized values by binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{Float64,1}:\n",
       " 0.295              \n",
       " 0.3                \n",
       " 0.30333333333333334\n",
       " 0.30499999999999994\n",
       " 0.33166666666666655\n",
       " 0.23499999999999996\n",
       " 0.2433333333333333 \n",
       " 0.32333333333333336\n",
       " 0.29166666666666663\n",
       " 0.29               \n",
       " 0.26999999999999996\n",
       " 0.34               \n",
       " 0.27               \n",
       " ⋮                  \n",
       " 0.24833333333333332\n",
       " 0.27499999999999997\n",
       " 0.30666666666666664\n",
       " 0.31499999999999995\n",
       " 0.36166666666666664\n",
       " 0.28               \n",
       " 0.24666666666666665\n",
       " 0.3433333333333334 \n",
       " 0.3066666666666667 \n",
       " 0.34               \n",
       " 0.32166666666666666\n",
       " 0.29833333333333334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics\n",
    "Nsample = 10000\n",
    "Nbin = 100\n",
    "Nbinsize = Nsample ÷ Nbin\n",
    "iter = Iterators.partition(Iterators.take(mcstep, Nsample), Nbinsize)\n",
    "bin = collect(map(mean, iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30245 ± 0.0037873063877276957\n"
     ]
    }
   ],
   "source": [
    "m = mean(bin)\n",
    "s = stdm(bin, m) / sqrt(length(bin))\n",
    "println(\"$m ± $s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nbinsize has to be determined based on the autocorrelation. In order to reduce Nbin to get a more acculate result, we need to implement some global updating algorithm.\n",
    "\n",
    "## Delete-1 jackknife resampling\n",
    "\n",
    "Delete-1 jackknife resampling is simply implemented in https://github.com/ararslan/Jackknife.jl. However, the function is limited, so I will newly define functions for the jackknife resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdJ (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This function mimics mapreduce but op should be any functions which\n",
    "returns one value from a vector.\n",
    "\"\"\"\n",
    "function leaveoneout(f::Function, op::Function, v::AbstractVector)\n",
    "    ind = eachindex(v)\n",
    "    map(i -> op(map(f, view(v, filter(!isequal(i), ind)))), ind)\n",
    "end\n",
    "meanJ(f::Function, op::Function, v::AbstractVector) = mean(leaveoneout(f, op, v))\n",
    "stdmJ(f::Function, op::Function, v::AbstractVector, m) = stdm(leaveoneout(f, op, v), m, corrected = false) * sqrt(length(v) - 1)\n",
    "stdJ(f::Function, op::Function, v::AbstractVector) = stdmJ(f, op, v, meanJ(f, op, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions are based on Statistics.jl and Jackknife.jl, so please see their reference to know how it works. I again use measurementEf for the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementEf (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@resumable function measurementEf(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Vector{Float64}\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 0.5im * J\n",
    "        h[nn[2], nn[1]] = -0.5im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0\n",
    "    β₂ = β * 0.5\n",
    "    hdense = Array(h)\n",
    "    plaq = collect(plaquette)\n",
    "    Np = length(plaq)\n",
    "    ev = zeros(Float64, N)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            evnew = eigvals(Hermitian(hdense))\n",
    "            βFnew = -sum(@. log(exp(β₂ * evnew[(N >> 1 + 1) : end]) + exp(-β₂ * evnew[(N >> 1 + 1) : end])))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "                ev .= evnew\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        Ef = -sum(@. ev[(N >> 1 + 1) : end] * tanh(β₂ * ev[(N >> 1 + 1) : end] )) * 0.5\n",
    "        ∂Ef∂β = -sum(@. (ev[(N >> 1 + 1) : end] ^ 2) * (sech(β₂ * ev[(N >> 1 + 1) : end]) ^ 2)) * 0.25\n",
    "        @yield [Ef, ∂Ef∂β]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ok to assume the bin size to be 1 for this problem. Thus, I omit Iterators.partition here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Array{Float64,1},1}:\n",
       " [-1.6762, -0.0454922] \n",
       " [-1.67994, -0.0460358]\n",
       " [-1.69272, -0.0482142]\n",
       " [-1.68707, -0.0470462]\n",
       " [-1.68707, -0.0470462]\n",
       " [-1.68042, -0.0454945]\n",
       " [-1.6847, -0.0468573] \n",
       " [-1.68091, -0.0457212]\n",
       " [-1.6723, -0.0448412] \n",
       " [-1.69037, -0.0474662]\n",
       " [-1.6793, -0.045817]  \n",
       " [-1.66923, -0.0439858]\n",
       " [-1.68973, -0.0471662]\n",
       " ⋮                     \n",
       " [-1.67659, -0.0456538]\n",
       " [-1.68438, -0.0468955]\n",
       " [-1.69643, -0.0487761]\n",
       " [-1.68723, -0.0471268]\n",
       " [-1.68658, -0.0466584]\n",
       " [-1.69136, -0.0477004]\n",
       " [-1.69295, -0.0483297]\n",
       " [-1.67074, -0.0444242]\n",
       " [-1.68534, -0.0471164]\n",
       " [-1.67935, -0.0458393]\n",
       " [-1.67246, -0.0441295]\n",
       " [-1.67686, -0.0450137]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcstep2 = Iterators.drop(measurementEf(Metropolis, openhoneycomb, 10.0, 4, 4), 10000)\n",
    "iter2 = Iterators.take(mcstep2, Nsample)\n",
    "data = collect(iter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting f = identity (i.e. doing nothing for data) and op = mean, meanJ and stdmJ work in the same way as mean and stdm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ef = -1.6833559836793284 ± 7.569447076947603e-5, ∂Ef∂β = -0.04645930869265934 ± 1.3418194240851154e-5\n"
     ]
    }
   ],
   "source": [
    "m2 = meanJ(identity, mean, data)\n",
    "s2 = stdmJ(identity, mean, data, m2)\n",
    "println(\"Ef = $(m2[1]) ± $(s2[1]), ∂Ef∂β = $(m2[2]) ± $(s2[2])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agrees with the standard estimation method for the error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 7.569447076955521e-5 \n",
       " 1.3418194240831252e-5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std(data) / sqrt(length(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such mean values (i.e. op = mean), the jackknife resampling is apparently overkill. However, to estimate the error for the values like the specific heat, the jackknife resampling is very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the Jackknife method already includes the bias coming from the autocorrelation, there is no need to think about the autocorrelation in this case. However, the autocorrelation prevents the error from decreacing rapidly, so it is still important to estimate the autocorrelation length.\n",
    "\n",
    "## Autocorrelation\n",
    "\n",
    "The simplest way to conider autocorrelation is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
