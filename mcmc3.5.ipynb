{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC3.5: Jackknife Resampling\n",
    "\n",
    "To estimate the error of observables accurately, it is recommended to use the Jackknife resampling method.\n",
    "\n",
    "## Binning\n",
    "\n",
    "Previously, we faced a large errorbar problem at low temperature in this program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementflux (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ResumableFunctions\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "const Jx = 1 / 3 # oppposite sign to Motome's\n",
    "const Jy = 1 / 3\n",
    "const Jz = 1 / 3\n",
    "function Metropolis(βF::Float64, βFnew::Float64)::Bool\n",
    "    βF - βFnew > log(rand())\n",
    "end\n",
    "function openhoneycomb(Lx::Int64, Ly::Int64)::Tuple\n",
    "    N = 2Lx * Ly\n",
    "    nnx = zip(1 : 2 : (N - 1), 2 : 2 : N)\n",
    "    nny = Iterators.flatten(zip((1 + 2i) : 2Lx : (2Lx * (Ly - 1)  + 1 + 2i), 2i : 2Lx : (2Lx * (Ly - 1)  + 2i)) for i in 1 : (Lx - 1))\n",
    "    nnz = zip(1 : 2 : (N - 1), Iterators.flatten(((2Lx + 2) : 2 : N, 2 : 2 : 2Lx)))\n",
    "    plaquette = Iterators.flatten(zip((Lx * (i - 1) + 1) : (Lx * (i - 1) + Lx - 1), (Lx * (i - 1) + 2) : (Lx * (i - 1) + Lx)) for i in 1 : Ly)\n",
    "    N, nnx, nny, nnz, plaquette\n",
    "end\n",
    "@resumable function measurementflux(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Float64\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 0.5im * J\n",
    "        h[nn[2], nn[1]] = -0.5im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0\n",
    "    hdense = Array(h)\n",
    "    plaq = collect(plaquette)\n",
    "    Np = length(plaq)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            ev = eigvals(Hermitian(hdense))\n",
    "            positiveev = Iterators.drop(ev, N >> 1)\n",
    "            βFnew = -sum((log(2.0 * cosh(β * ϵ / 2.0)) for ϵ in positiveev))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        @yield sum((η[k] * η[l] for (k, l) in plaq)) / Np\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One reason is the discreteness of the returned value \"flux.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.5\n",
      "0.16666666666666666\n",
      "0.3333333333333333\n",
      "-0.3333333333333333\n",
      "0.16666666666666666\n",
      "-0.16666666666666666\n",
      "0.0\n",
      "-0.16666666666666666\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "mcstep = Iterators.drop(measurementflux(Metropolis, openhoneycomb, 100.0, 4, 4), 10000)\n",
    "foreach(println, Iterators.take(mcstep, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to flatten these quantized values by binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{Float64,1}:\n",
       " 0.25166666666666665\n",
       " 0.33833333333333326\n",
       " 0.22500000000000003\n",
       " 0.2833333333333333 \n",
       " 0.3083333333333333 \n",
       " 0.305              \n",
       " 0.30833333333333335\n",
       " 0.30833333333333335\n",
       " 0.3566666666666667 \n",
       " 0.32166666666666666\n",
       " 0.35166666666666657\n",
       " 0.32               \n",
       " 0.27666666666666667\n",
       " ⋮                  \n",
       " 0.295              \n",
       " 0.31               \n",
       " 0.32833333333333337\n",
       " 0.29               \n",
       " 0.37666666666666665\n",
       " 0.3416666666666666 \n",
       " 0.30666666666666664\n",
       " 0.26166666666666666\n",
       " 0.32               \n",
       " 0.29666666666666663\n",
       " 0.32833333333333337\n",
       " 0.3399999999999999 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics\n",
    "Nsample = 10000\n",
    "Nbin = 100\n",
    "Nbinsize = Nsample ÷ Nbin\n",
    "iter = Iterators.partition(Iterators.take(mcstep, Nsample), Nbinsize)\n",
    "bin = collect(map(mean, iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30755 ± 0.0036713663636895426\n"
     ]
    }
   ],
   "source": [
    "m = mean(bin)\n",
    "s = stdm(bin, m) / sqrt(length(bin))\n",
    "println(\"$m ± $s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nbinsize has to be determined based on the autocorrelation. In order to reduce Nbin to get a more acculate result, we need to implement some global updating algorithm.\n",
    "\n",
    "## Delete-1 jackknife resampling\n",
    "\n",
    "Delete-1 jackknife resampling is simply implemented in https://github.com/ararslan/Jackknife.jl. However, the function is limited, so I will newly define functions for the jackknife resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdJ (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function leaveoneout(before::Function, after::Function, v::AbstractVector)\n",
    "    ind = eachindex(v)\n",
    "    map(i -> after(mean(map(before, view(v, filter(!isequal(i), ind))))), ind)\n",
    "end\n",
    "meanJ(b::Function, a::Function, v::AbstractVector) = mean(leaveoneout(b, a, v))\n",
    "stdmJ(b::Function, a::Function, v::AbstractVector, m) = stdm(leaveoneout(b, a, v), m, corrected = false) * sqrt(length(v) - 1)\n",
    "stdJ(b::Function, a::Function, v::AbstractVector) = stdmJ(b, a, v, meanJ(b, a, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions are based on Statistics.jl and Jackknife.jl, so please see their reference to know how it works. I again use measurementEf for the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementEf (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@resumable function measurementEf(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Vector{Float64}\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 0.5im * J\n",
    "        h[nn[2], nn[1]] = -0.5im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0\n",
    "    β₂ = β * 0.5\n",
    "    hdense = Array(h)\n",
    "    plaq = collect(plaquette)\n",
    "    Np = length(plaq)\n",
    "    ev = zeros(Float64, N)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            evnew = eigvals(Hermitian(hdense))\n",
    "            βFnew = -sum(@. log(exp(β₂ * evnew[(N >> 1 + 1) : end]) + exp(-β₂ * evnew[(N >> 1 + 1) : end])))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "                ev .= evnew\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        Ef = -sum(@. ev[(N >> 1 + 1) : end] * tanh(β₂ * ev[(N >> 1 + 1) : end] )) * 0.5\n",
    "        ∂Ef∂β = -sum(@. (ev[(N >> 1 + 1) : end] ^ 2) * (sech(β₂ * ev[(N >> 1 + 1) : end]) ^ 2)) * 0.25\n",
    "        @yield [Ef, ∂Ef∂β]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is ok to first assume the bin size to be 1 for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Array{Float64,1},1}:\n",
       " [-1.68042, -0.0454945]\n",
       " [-1.68826, -0.0473381]\n",
       " [-1.665, -0.0431975]  \n",
       " [-1.69243, -0.0481061]\n",
       " [-1.68162, -0.0458104]\n",
       " [-1.68441, -0.0469445]\n",
       " [-1.68104, -0.045745] \n",
       " [-1.68561, -0.046466] \n",
       " [-1.67757, -0.0450611]\n",
       " [-1.69541, -0.0485065]\n",
       " [-1.68162, -0.0458104]\n",
       " [-1.6796, -0.0459299] \n",
       " [-1.69361, -0.0483618]\n",
       " ⋮                     \n",
       " [-1.68707, -0.0470462]\n",
       " [-1.66452, -0.0429756]\n",
       " [-1.68444, -0.0469634]\n",
       " [-1.67332, -0.0445042]\n",
       " [-1.67932, -0.0457827]\n",
       " [-1.68615, -0.0466634]\n",
       " [-1.67768, -0.0453661]\n",
       " [-1.67805, -0.0454819]\n",
       " [-1.68427, -0.0469116]\n",
       " [-1.68399, -0.0465644]\n",
       " [-1.68104, -0.045745] \n",
       " [-1.69321, -0.048395] "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const β = 10.0\n",
    "mcstep2 = Iterators.drop(measurementEf(Metropolis, openhoneycomb, β, 4, 4), 10000)\n",
    "iter2 = Iterators.take(mcstep2, Nsample)\n",
    "data = collect(iter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting after = before = identity, meanJ and stdmJ work in the same way as mean and stdm, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ef = -1.6833612268417353 ± 7.416156890922378e-5, ∂Ef∂β = -0.0464616986607695 ± 1.3158376480010395e-5\n"
     ]
    }
   ],
   "source": [
    "m2 = meanJ(identity, identity, data)\n",
    "s2 = stdmJ(identity, identity, data, m2)\n",
    "println(\"Ef = $(m2[1]) ± $(s2[1]), ∂Ef∂β = $(m2[2]) ± $(s2[2])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agrees with the standard estimation method for the error bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 7.416156890897991e-5\n",
       " 1.315837648002303e-5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std(data) / sqrt(length(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For such mean values (i.e. op = mean), the jackknife resampling is apparently overkill. However, to estimate the error for the values like the specific heat, the jackknife resampling is very effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cv = 4.65166925433098 ± 0.001323276237596492\n"
     ]
    }
   ],
   "source": [
    "TTCv(v::Vector{Float64}) = [v[1] ^ 2 - v[2], v[1]]\n",
    "Cv(meanTTCv::Vector{Float64}) = (β ^ 2) * (meanTTCv[1] - meanTTCv[2] ^ 2)\n",
    "m3 = meanJ(TTCv, Cv, data)\n",
    "s3 = stdmJ(TTCv, Cv, data, m3)\n",
    "println(\"Cv = $m3 ± $s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "The simplest way to estimate autocorrelation is by changing the size of binning and estimating its errors by jackknife resampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000-element Array{Array{Float64,1},1}:\n",
       " [-1.68434, -0.0464163]\n",
       " [-1.67871, -0.0456518]\n",
       " [-1.68302, -0.0463774]\n",
       " [-1.68332, -0.0461055]\n",
       " [-1.68649, -0.0467838]\n",
       " [-1.68061, -0.0458702]\n",
       " [-1.68946, -0.0477477]\n",
       " [-1.68109, -0.0460505]\n",
       " [-1.68057, -0.0458423]\n",
       " [-1.68067, -0.0459857]\n",
       " [-1.68146, -0.0465996]\n",
       " [-1.68538, -0.0471572]\n",
       " [-1.68998, -0.0475709]\n",
       " ⋮                     \n",
       " [-1.678, -0.0457511]  \n",
       " [-1.68925, -0.0473677]\n",
       " [-1.68163, -0.0458801]\n",
       " [-1.68356, -0.046485] \n",
       " [-1.67763, -0.0456946]\n",
       " [-1.68292, -0.0462062]\n",
       " [-1.67579, -0.0450109]\n",
       " [-1.67888, -0.0457338]\n",
       " [-1.68273, -0.046223] \n",
       " [-1.67787, -0.045424] \n",
       " [-1.68413, -0.046738] \n",
       " [-1.68712, -0.04707]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binning = Iterators.partition(data, 2)\n",
    "bin2 = collect(map(mean, binning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cv = 4.64899491259832 ± 0.0013320820507892847\n"
     ]
    }
   ],
   "source": [
    "m4 = meanJ(TTCv, Cv, bin2)\n",
    "s4 = stdmJ(TTCv, Cv, bin2, m4)\n",
    "println(\"Cv = $m4 ± $s4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the binsize does not affect the expectation value (or the errorbar) too much means that the autocorrelation length is about 1-2. Note that at low temperature the binsize strongly affects the expectation value, which means that the binsize must be taken to be large enough. Here we recalculate the autocorrelation length by another method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
