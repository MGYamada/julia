{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC3.5: Jackknife Resampling\n",
    "\n",
    "To estimate the error of observables accurately, it is recommended to use the Jackknife resampling method.\n",
    "\n",
    "## Binning\n",
    "\n",
    "Previously, we faced a large errorbar problem in this program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementflux (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ResumableFunctions\n",
    "using SparseArrays\n",
    "using LinearAlgebra\n",
    "const Jx = 1 / 3 # oppposite sign to Motome's\n",
    "const Jy = 1 / 3\n",
    "const Jz = 1 / 3\n",
    "function Metropolis(βF::Float64, βFnew::Float64)::Bool\n",
    "    βF - βFnew > log(rand())\n",
    "end\n",
    "function openhoneycomb(Lx::Int64, Ly::Int64)::Tuple\n",
    "    N = 2Lx * Ly\n",
    "    nnx = zip(1 : 2 : (N - 1), 2 : 2 : N)\n",
    "    nny = Iterators.flatten(zip((1 + 2i) : 2Lx : (2Lx * (Ly - 1)  + 1 + 2i), 2i : 2Lx : (2Lx * (Ly - 1)  + 2i)) for i in 1 : (Lx - 1))\n",
    "    nnz = zip(1 : 2 : (N - 1), Iterators.flatten(((2Lx + 2) : 2 : N, 2 : 2 : 2Lx)))\n",
    "    plaquette = Iterators.flatten(zip((Lx * (i - 1) + 1) : (Lx * (i - 1) + Lx - 1), (Lx * (i - 1) + 2) : (Lx * (i - 1) + Lx)) for i in 1 : Ly)\n",
    "    N, nnx, nny, nnz, plaquette\n",
    "end\n",
    "@resumable function measurementflux(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Float64\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 0.5im * J\n",
    "        h[nn[2], nn[1]] = -0.5im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0\n",
    "    hdense = Array(h)\n",
    "    plaq = collect(plaquette)\n",
    "    Np = length(plaq)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            ev = eigvals(Hermitian(hdense))\n",
    "            positiveev = Iterators.drop(ev, N >> 1)\n",
    "            βFnew = -sum((log(2.0 * cosh(β * ϵ / 2.0)) for ϵ in positiveev))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        @yield sum((η[k] * η[l] for (k, l) in plaq)) / Np\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is due to the discreteness of the returned value \"flux.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "0.16666666666666666\n",
      "0.16666666666666666\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.16666666666666666\n",
      "0.16666666666666666\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "mcstep = Iterators.drop(measurementflux(Metropolis, openhoneycomb, 100.0, 4, 4), 10000)\n",
    "foreach(println, Iterators.take(mcstep, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to flatten these quantized value by binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100-element Array{Float64,1}:\n",
       " 0.3016666666666667 \n",
       " 0.2683333333333333 \n",
       " 0.28333333333333327\n",
       " 0.33833333333333326\n",
       " 0.29833333333333334\n",
       " 0.26               \n",
       " 0.2866666666666666 \n",
       " 0.3116666666666667 \n",
       " 0.28833333333333333\n",
       " 0.24666666666666667\n",
       " 0.27666666666666667\n",
       " 0.32166666666666666\n",
       " 0.2383333333333333 \n",
       " ⋮                  \n",
       " 0.3549999999999999 \n",
       " 0.3899999999999999 \n",
       " 0.3316666666666667 \n",
       " 0.2833333333333333 \n",
       " 0.30166666666666664\n",
       " 0.26666666666666666\n",
       " 0.285              \n",
       " 0.24333333333333332\n",
       " 0.27666666666666667\n",
       " 0.2483333333333333 \n",
       " 0.2533333333333333 \n",
       " 0.3483333333333334 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Statistics\n",
    "Nsample = 10000\n",
    "Nbin = 100\n",
    "Nbinsize = Nsample ÷ Nbin\n",
    "iter = Iterators.partition(Iterators.take(mcstep, Nsample), Nbinsize)\n",
    "bin = collect(map(mean, iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29983333333333334 ± 0.03313535482861781\n"
     ]
    }
   ],
   "source": [
    "m = mean(bin)\n",
    "s = stdm(bin, m)\n",
    "println(\"$m ± $s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nbinsize has to be determined based on the autocorrelation.\n",
    "\n",
    "## Delete-1 Jackknife resampling\n",
    "\n",
    "Delete-1 Jackknife resampling is simply implemented in Jackknife.jl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "measurementEf (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@resumable function measurementEf(method::Function, lattice::Function, β::Float64, Lx::Int64, Ly::Int64)::Vector{Float64}\n",
    "    N, nnx, nny, nnz, plaquette = lattice(Lx, Ly)\n",
    "    iter = Iterators.flatten((Iterators.product(J, nn) for (J, nn) in [(Jx, nnx), (Jy, nny), (Jz, nnz)]))\n",
    "    h = spzeros(Complex{Float64}, N, N)\n",
    "    for (J, nn) in iter\n",
    "        h[nn[1], nn[2]] = 0.5im * J\n",
    "        h[nn[2], nn[1]] = -0.5im * J\n",
    "    end\n",
    "    NNz = collect(nnz)\n",
    "    Nz = length(NNz)\n",
    "    η = ones(Int64, Nz)\n",
    "    βF = 0.0 # any positive number is ok\n",
    "    β₂ = β * 0.5\n",
    "    hdense = Array(h)\n",
    "    plaq = collect(plaquette)\n",
    "    Np = length(plaq)\n",
    "    ev = zeros(Float64, N)\n",
    "    while true\n",
    "        for i in 1 : Nz\n",
    "            j = rand(1 : Nz)\n",
    "            hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "            hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            evnew = eigvals(Hermitian(hdense))\n",
    "            βFnew = -sum(@. log(exp(β₂ * evnew[(N >> 1 + 1) : end]) + exp(-β₂ * evnew[(N >> 1 + 1) : end])))\n",
    "            if method(βF, βFnew)\n",
    "                η[j] = -η[j]\n",
    "                βF = βFnew\n",
    "                ev .= evnew # be careful \".\" is necessary\n",
    "            else\n",
    "                hdense[NNz[j][1], NNz[j][2]] = -hdense[NNz[j][1], NNz[j][2]]\n",
    "                hdense[NNz[j][2], NNz[j][1]] = -hdense[NNz[j][2], NNz[j][1]]\n",
    "            end\n",
    "        end\n",
    "        Ef = -sum(@. ev[(N >> 1 + 1) : end] * tanh(β₂ * ev[(N >> 1 + 1) : end] )) * 0.5\n",
    "        ∂Ef∂β = -sum(@. (ev[(N >> 1 + 1) : end] ^ 2) * (sech(β₂ * ev[(N >> 1 + 1) : end]) ^ 2)) * 0.25\n",
    "        # you can also use automatic differentiation\n",
    "        @yield [Ef, ∂Ef∂β] # Tuple does not work in Statistics.jl\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
