{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC1.0: Introduction to Markov Chain Monte Carlo\n",
    "\n",
    "The aim of the Markov chain Monte Carlo (MCMC) is to produce random variables obeying any probability distributions you like. There are three main tools for MCMC and I will introduce them one by one.\n",
    "\n",
    "* Metropolis-Hastings algorithm\n",
    "* Gibbs sampler\n",
    "* Hybrid Monte Carlo\n",
    "\n",
    "## Metropolis-(Hastings) algorithm\n",
    "\n",
    "The Metropolis-Hastings (MH) algorithm is the most important and most popular tool in MCMC, Bayesian statistics, and physics simulations. It is universal and applicable to any kinds of distributions. The other methods usually have some restrictions, so if you do not know how to use another one like a Gibbs sampler, you should choose the MH method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Monte Carlo simulation for Ising model\n",
    "\n",
    "As a typical example, I will present a simple Julia implimentation of the Monte Carlo simulation of the 2D Ising model on the square lattice. The Metropolis rule is simply given by the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metropolis (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Metropolis(β::Float64, ΔE::Float64)::Bool\n",
    "    (ΔE <= 0.0) || (exp(-β * ΔE) > rand())\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will tell us whether we will accept a new state or not according to its energy gain $\\Delta E$. It is clear that acceptance/rejection will be determined probabilistically and typically the MH alrorithm has an acceptance rate of only 20-40% (c.f. in the Ising model it is much higher). I will later show you algorithms with an almost 100% acceptance rate, such as a Gibbs sampler or a hybrid Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing spin configurations\n",
    "\n",
    "The most memory-effcient way to store a spin configuration is using bitwise operations. Usually we need a very low-level code for such bitwise operations (see e.g. \"Hacker's Delight\" by H. S. Warren), but Julia provides a nice platform called BitArray. We regard a bit 1 as spin-up ($\\sigma =+1$) and 0 as spin-down ($\\sigma =-1$). For 1D, we can just store an array of spins as BitArray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element BitArray{1}:\n",
       " true\n",
       " true\n",
       " true\n",
       " true\n",
       " true\n",
       " true\n",
       " true\n",
       " true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 8\n",
    "spin1D = trues(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to store a 2D spin configuration, Array of BitArray is more useful than 2D BitArray (you will understand later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{BitArray{1},1}:\n",
       " [true, false, false, true, true, true, true, true]   \n",
       " [false, true, true, true, true, true, true, false]   \n",
       " [true, true, false, false, true, true, false, false] \n",
       " [true, false, false, true, false, false, true, false]\n",
       " [true, true, true, true, false, true, false, true]   \n",
       " [false, true, true, true, true, true, true, false]   \n",
       " [true, false, true, true, false, true, true, true]   \n",
       " [false, true, true, true, false, true, true, false]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spin2D = rand(Bool, L, L)\n",
    "spin2Drow = [BitVector(spin2D[n, :]) for n in 1 : L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the future purpose, it is useful later to store the transposed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{BitArray{1},1}:\n",
       " [true, false, true, true, true, false, true, false]  \n",
       " [false, true, true, false, true, true, false, true]  \n",
       " [false, true, false, false, true, true, true, true]  \n",
       " [true, true, false, true, true, true, true, true]    \n",
       " [true, true, true, false, false, true, false, false] \n",
       " [true, true, true, false, true, true, true, true]    \n",
       " [true, true, false, true, false, true, true, true]   \n",
       " [true, false, false, false, true, false, true, false]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spin2Dcolumn = [BitVector(spin2D[:, n]) for n in 1 : L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating\n",
    "\n",
    "Assuming a periodic boundary condition, the nearest-neighbor sites can be obtained by bitwise shift (rotation) operations. It is appropriate for the small $L$ but for a larger size it has a large overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = -1.0 # Ferro\n",
    "function energygain(x::Int64, y::Int64, row::Vector{BitVector}, column::Vector{BitVector})::Float64\n",
    "    left = xor(circshift(row[y], 1)[x], row[y][x])\n",
    "    right = xor(circshift(row[y], -1)[x], row[y][x])\n",
    "    top = xor(circshift(column[x], 1)[y], column[x][y])\n",
    "    bottom = xor(circshift(column[x], -1)[y], column[x][y])\n",
    "    -2. * J * sum(@. 1 - 2 * [left, right, top, bottom]) # This is not a smart way\n",
    "end\n",
    "energygain(3, 1, spin2Drow, spin2Dcolumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we choose the flipping spin sequentially (sequential sweep), it may destroy a detailed balance microscopically. Usually it is better to choose a candidate spin using a random variable. Note that functions can be used as arguments in Juila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5, true)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function step!(β::Float64, method::Function, row::Vector{BitVector}, column::Vector{BitVector})::Tuple\n",
    "    x = rand(1 : L)\n",
    "    y = rand(1 : L)\n",
    "    ΔE = energygain(x, y, row, column)\n",
    "    flip = method(β, ΔE)\n",
    "    row[y][x] = xor(row[y][x], flip)\n",
    "    column[x][y] = xor(column[x][y], flip)\n",
    "    x, y, flip # Actually, it is not recommeded for destructive functions to return variables \n",
    "end\n",
    "step!(0.1, Metropolis, spin2Drow, spin2Dcolumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is using \"call by reference\" for Array, so spin2Drow will be directely updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{BitArray{1},1}:\n",
       " [true, false, false, true, true, true, true, true]   \n",
       " [false, true, true, true, true, true, true, false]   \n",
       " [true, true, false, false, true, true, false, false] \n",
       " [true, false, false, true, false, false, true, false]\n",
       " [true, true, true, true, false, false, false, true]  \n",
       " [false, true, true, true, true, true, true, false]   \n",
       " [true, false, true, true, false, true, true, true]   \n",
       " [false, true, true, true, false, true, true, false]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spin2Drow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "Let's warm up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{BitArray{1},1}:\n",
       " [false, true, false, false, false, true, false, false]\n",
       " [true, false, false, true, true, true, false, true]   \n",
       " [true, false, false, true, false, false, false, true] \n",
       " [true, false, false, false, true, true, false, false] \n",
       " [false, true, false, true, true, false, true, true]   \n",
       " [false, false, true, false, true, false, false, false]\n",
       " [false, true, false, false, false, false, true, true] \n",
       " [false, true, true, false, false, true, true, false]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function warmup!(β::Float64, spin2Drow::Vector{BitVector}, spin2Dcolumn::Vector{BitVector})\n",
    "    N = 100000\n",
    "    for i in 1 : N\n",
    "        step!(β, Metropolis, spin2Drow, spin2Dcolumn)\n",
    "    end\n",
    "end\n",
    "warmup!(0.1, spin2Drow, spin2Dcolumn)\n",
    "spin2Drow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At low temperature, the spins will be ordered to the ferromagnetic state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{BitArray{1},1}:\n",
       " [false, false, false, false, false, false, false, false]\n",
       " [false, false, false, false, false, false, false, false]\n",
       " [false, false, false, false, false, false, false, false]\n",
       " [false, false, false, false, false, false, false, false]\n",
       " [false, false, false, false, false, false, false, false]\n",
       " [false, false, false, false, false, false, false, false]\n",
       " [false, false, false, false, false, false, false, false]\n",
       " [false, false, false, false, false, false, false, false]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup!(10.0, spin2Drow, spin2Dcolumn)\n",
    "spin2Drow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs sampler (heat-bath method)\n",
    "\n",
    "A Gibbs sampler is the most powerful tool in MCMC if you are able to generate a \"conditional probability distribution.\" Only in this case, we can achieve a 100% acceptance rate for the continuous variable distributions, i.e. the acceptance/rejection process is deterministic. For some reason a Gibbs sampler is called heat-bath method in physics, but you should learn it in this generalized form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Assuming we want to sample from a distribution defined by a probablity\n",
    "$$\n",
    "p(x,y) \\propto e^\\frac{-a x^2+2b xy - cy^2}{2},\n",
    "$$\n",
    "we can explicitly derive a conditional distribution as a Gaussian distribution for each variable.\n",
    "$$\n",
    "p(x|y) \\propto e^{-\\frac{a}{2} \\left(x - \\frac{b}{a}y\\right)^2}, \\\\\n",
    "p(y|x) \\propto e^{-\\frac{c}{2} \\left(y - \\frac{b}{c}x\\right)^2}.\n",
    "$$\n",
    "\n",
    "By updating $x$ and $y$ alternately from these two conditional distributions, we can produce the original joint distribution because it is the invariant distribution of these two sampling processes. Note that the detailed balance is microscopically broken, but the algorithm is balanced as a total. If you wish to resotre it, it is better to choose $x$ or $y$ randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.239921547930485, -0.5389071016521685)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions\n",
    "function Gibbssampler(a::Float64, b::Float64, c::Float64, x::Float64, y::Float64)::Tuple{Float64, Float64}\n",
    "    x = rand(Normal(b / a * y, 1 / sqrt(a)))\n",
    "    y = rand(Normal(b / c * x, 1 / sqrt(c)))\n",
    "    x, y\n",
    "end\n",
    "Gibbssampler(1.0, 0.8, 1.0, 0.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia includes the library for various distributions, so the implementation should be simple. After the convergence it will give us a correct distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4499480181974609, -0.5149621718441911)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sampling(a::Float64, b::Float64, c::Float64)::Tuple{Float64, Float64}\n",
    "    N = 100000\n",
    "    x = 0.0\n",
    "    y = 0.0\n",
    "    for i in 1 : N\n",
    "        x, y = Gibbssampler(a, b, c, x, y)\n",
    "    end\n",
    "    x, y\n",
    "end\n",
    "sampling(1.0, 0.8, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Ising model\n",
    "\n",
    "For the Ising model, the Gibbs sampler will reduce to just a so-called heat-bath method, which has been named probably because the unupdated spins can be regarded as a heat bath and the transition probablities obey the partial \"canonical distribution\" for the updated spin. The implementation is simple and can be done just by replacing the function Metropolis to the following one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 1, true)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function heatbath(β::Float64,ΔE::Float64)::Bool\n",
    "    (1.0 / (exp(β * ΔE) + 1.0)) > rand()\n",
    "end\n",
    "step!(0.1, heatbath, spin2Drow, spin2Dcolumn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the acceptance rate is apparently not 100% due to the finite number of the possible transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{BitArray{1},1}:\n",
       " [true, false, true, false, false, true, true, false]  \n",
       " [true, false, true, false, false, false, false, false]\n",
       " [true, true, false, false, false, false, false, false]\n",
       " [true, false, true, false, false, true, false, true]  \n",
       " [false, false, true, false, true, true, true, true]   \n",
       " [true, false, false, false, true, true, true, true]   \n",
       " [true, true, true, false, true, false, false, false]  \n",
       " [false, true, true, true, true, true, true, true]     "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function heatup!(β::Float64, spin2Drow::Vector{BitVector}, spin2Dcolumn::Vector{BitVector})\n",
    "    N = 100000\n",
    "    for i in 1 : N\n",
    "        step!(β, heatbath, spin2Drow, spin2Dcolumn)\n",
    "    end\n",
    "end\n",
    "heatup!(0.1, spin2Drow, spin2Dcolumn)\n",
    "spin2Drow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{BitArray{1},1}:\n",
       " [true, true, true, true, true, true, true, true]\n",
       " [true, true, true, true, true, true, true, true]\n",
       " [true, true, true, true, true, true, true, true]\n",
       " [true, true, true, true, true, true, true, true]\n",
       " [true, true, true, true, true, true, true, true]\n",
       " [true, true, true, true, true, true, true, true]\n",
       " [true, true, true, true, true, true, true, true]\n",
       " [true, true, true, true, true, true, true, true]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatup!(10.0, spin2Drow, spin2Dcolumn)\n",
    "spin2Drow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blocked Gibbs sampler\n",
    "\n",
    "From now on, I will introduce some variants of the original Gibbs sampler. If we have 3 or more parameters for the Gibbs sampler, then we can group them into 2 or more groups.\n",
    "\n",
    "~ under construction ~\n",
    "\n",
    "### Collapsed Gibbs sampler\n",
    "\n",
    "We can also integrate out some parameter to get a collapsed Gibbs sampler.\n",
    "\n",
    "~ under construction ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Monte Carlo\n",
    "\n",
    "Condensed matter physicists do not usually use this method because they are mostly using discrete variables, but this method is the most efficient method with an almost 100% acceptance rate \"for continuous variables\" because it is using a gradient of the probability distribution. A hybrid Monte Carlo or Hamiltonian Monte Carlo (HMC) would be the best solution for the distribution with a known gradient and it is much more efficient than the pure MH method because of the highest acceptance rate.\n",
    "\n",
    "The essence of HMC can be understood as some hybrid of the MH method and the Gibbs sampler (or deterministic molecular dynamics), and HMC generates an extended probability distribution including an auxiliary field called \"momentum,\" where the conditional probability distribution without this momentum becomes the disired one. In some quantum Monte Carlo (QMC) simulations, it is difficult to calculate the gradient explicitly, and then we have to introduce another auxiliary field (called \"pseudofermion\" although it is actually bosonic) to estimate the gradient probabilistically \"as an avarage.\"\n",
    "\n",
    "### How it works\n",
    "\n",
    "Instead of generating the original distribution $\\exp(-V(\\chi))$ (e.g. $V(\\chi) = \\beta E(\\chi)$ for the continuous variables $\\chi$), we would like to generate a joint distribution of $\\chi$ and $\\pi$ (if $\\chi$ is a vector, $\\pi$ should have the same dimention):\n",
    "$$p(\\chi,\\pi)=\\exp(-\\pi^2-V(\\chi)).$$\n",
    "If we simply discard the auxiliary variables $\\pi$, the resulting conditional distribution simply becomes the original one.\n",
    "$$p(\\chi | \\pi)=\\exp(-V(\\chi)).$$\n",
    "Thus it is enough to generate the extended distribution with doubled variables.\n",
    "\n",
    "If we define a classical Hamiltonian as $\\mathcal{H}=\\pi^2+V(\\chi)$, the problem results in the generation of the distribution of the classical probabilistic system with $p=\\exp(-\\mathcal{H})$. Note that we simply set the mass $m=1/2$, but we can generalize it to any mass matrix.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "The updating process goes similarly to the Gibbs sampler. First, we update $\\pi$ from\n",
    "$$p(\\pi | \\chi)=\\exp(-\\pi^2).$$\n",
    "This is just a normal distribution, so Box–Muller's method simply works for this step.\n",
    "\n",
    "We use a deterministic molecular dynamics to update $\\chi$. Since the equation of motion conserves the total energy, the distribution is not changed by this motion from $t=0$ to any time $t=T$ when we update both $\\chi$ and $\\pi$ simultaneously according to the canonical equation. Typically, we move $(\\chi,\\pi)$ for $T=1$ according to the randomly selected momentum $\\pi$ in order to move $\\chi$ for a long enough distance (to avoid the self-correlation). The detailed balance completely holds for this deterministic updating *as long as the equation of motion is solved exactly without any numerical error.* Under this (numerically impossible) assumption, we can achieve 100% acceptance rate for this HMC algorithm.\n",
    "\n",
    "### Leap-flogging\n",
    "\n",
    "In the ideal case with a negligible numerical error, HMC can achieve almost 100% acceptance rate and in this sense it is very efficient, but this is impossible in a real computer using floating-point numbers. Thus, we use the following integrator for the equation of motion to keep its detailed balance:\n",
    "\n",
    "1. $\\chi(t+\\Delta t/2)  = \\chi(t) + \\pi(t) \\Delta t.$\n",
    "1. $\\pi(t+ \\Delta t) = \\pi(t) - \\nabla V(\\chi+\\Delta t/2) \\Delta t.$\n",
    "1. $\\chi(t+\\Delta t)  = \\chi(t+\\Delta t/2) + \\pi(t+ \\Delta t) \\Delta t.$\n",
    "\n",
    "This algorithm is apparently reversible, i.e. everything goes back when we replace $\\pi \\to -\\pi$. This fact will not be affected by the numerical error because the unupdated variable is conserved for each step (see e.g. the lifting algorithm for the wavelet transformation). Thus, the detailed balance (i.e. reversibility of the process) is still kept even with a numerical error for this leap-flogging approximation considering the fact that the normal distribution produces the same amount of $\\pi$ and $-\\pi$.\n",
    "\n",
    "However, if $\\Delta t$ is not small enough, the energy $\\mathcal{H}$ is no longer conserved due to the numerical error and we need an acceptance/rejection process to keep its distribution. This is simply realized by the MH algorithm for the accumulated error for the energy $\\Delta \\mathcal{H} = \\mathcal{H}_\\textrm{new} - \\mathcal{H}_\\textrm{old}$. Note that we are dealing with a probability density, not a probability itself, so the measure $d\\chi d\\pi$ of $p(\\chi,\\pi)d\\chi d\\pi$ also has to be conserved. The leap-flogging has no problem on this point because it clearly conserves the symplectic form $d\\chi d\\pi$ regardless of the error (take the exact differential of each step).\n",
    "\n",
    "If we begin from the MH algorithm (other way around), HMC can be regarded as an improved version of the random walk + MH algorithm. If we just do a random walk (with a momentum $\\pi$) and check it by MH, there is no reason to think that the energy difference $\\Delta \\mathcal{H} = \\mathcal{H}_\\textrm{new} - \\mathcal{H}_\\textrm{old}$ becomes small. However, if we use a molecular dynamics instead of the random walk, the origin of $\\Delta \\mathcal{H}$ is totally from the numerical error and we can expect that it becomes small if the calculation is accurate enough. HMC is one simple way to increase the acceptance rate of the MH algorithm to almost 100%.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leapflogging (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function leapflogging(χ, π, Δt::Float64, N::Int64, ∇V::Function)::Tuple\n",
    "    Δχ = π * Δt\n",
    "    χ += Δχ\n",
    "    for i in 1 : (N - 1)\n",
    "        π -= ∇V(χ) * Δt\n",
    "        χ += 2Δχ\n",
    "    end\n",
    "    π -= ∇V(χ) * Δt\n",
    "    χ += Δχ\n",
    "    χ, π\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 1.531460989643879 \n",
       " 3.3685009236667263"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "function HMC(χ::Vector{Float64}, V::Function, ∇V::Function)::Vector{Float64}\n",
    "    Nstep = 10000\n",
    "    Nintegrate = 10\n",
    "    Δt = 1 / Nintegrate\n",
    "    π = similar(χ) # Actually, it is not good to use π=3.14...\n",
    "    for i in 1 : Nstep\n",
    "        π .= rand(Normal(0., 1. / sqrt(2.)), length(π))\n",
    "        Hold = π ⋅ π + V(χ)\n",
    "        χold = χ\n",
    "        πold = π\n",
    "        χ, π = leapflogging(χ, π, Δt, Nintegrate, ∇V)\n",
    "        Hnew = dot(π, π) + V(χ)\n",
    "        #println((Hold, Hnew))\n",
    "        if Metropolis(1., Hnew - Hold)\n",
    "            #println(\"Accepted!\")\n",
    "        else\n",
    "            χ = χold\n",
    "            π = πold\n",
    "        end\n",
    "    end\n",
    "    χ\n",
    "end\n",
    "v = x -> (x[1] ^ 2 - 2 * 0.8x[1] * x[2] + x[2]^2) / 2.\n",
    "dv = x -> [x[1] - 0.8x[2], x[2] - 0.8x[1]]\n",
    "HMC([0.0, 0.0], v, dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation\n",
    "\n",
    "Now in the field of machine learning or Bayesian statistics, the \"autograd\" i.e. automatic differentiation becomes one of the most powerful tools. Physicists still like to do differentiations by themselves, but a functional language like Julia can differentiate the function directly, so we no longer need any paper/pen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 5.468502072637202 \n",
       " 1.6963115723574302"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "HMC([0.0, 0.0], v, x -> ForwardDiff.gradient(v, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/JuliaDiff/ForwardDiff.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
