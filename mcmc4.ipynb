{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC4.0: Replica Exchange Monte Carlo (Parallel Tempering)\n",
    "\n",
    "In order to get correct and unbiased results from QMC simulations, we need to implement a \"global updating\" algorithm in addition to the single-flip \"local updating.\" Here I try to rewrite the previous code using Channels to implement parallelization to show how it goes in Jupyter Notebook. However, in most cases ResumableFunctions.jl + MPI.jl is much faster, and you should use MPI.jl instead of using Channels. The implementation based on MPI.jl will be included in mcmc4.5.jl.\n",
    "\n",
    "## Autocorrelation\n",
    "\n",
    "It was already found (especially in MCMC3.5) that the autocorrelation length is a key feature determining the efficiency of MCMC. StatsBase.jl includes methods for the autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics\n",
    "using StatsBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decrease the autocorrelation length, we basically need to increase the acceptance rate.\n",
    "\n",
    "In the case of continuous variables, the acceptance rate can be increased by using a Gibbs sampler (100%) or a hybrid Monte Carlo (almost 100%). You can easily check the autocorrelation of the Gibbs sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, those algorithms usually do not work for discrete variables, which are important for physical applications. Another way to reduce the autocorrelation length is by implementing global updating. There are various technologies for global updating, but replica exchange is the most universal (in the same way as Metropolis-Hastings was universal for any probabilistic distributions).\n",
    "\n",
    "## Replica exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we forgot mentioning that we can simply increase the acceptance rate by changing the proposed distribution. This point will be discussed in MCMC6.5.\n",
    "\n",
    "## Kitaev model revisited\n",
    "\n",
    "There are two ways to exchange the distributions, one by exchanging the temperature, the other by exchanging the internal states. Apparently, the former is faster, but in the case of the Kitaev model the latter is still acceptable because the internal state is just labbeled by the flux sector and not memory-consumptive. I will implement by exchaging the states. *There is another reason for this decision, which you will see later.* First, let me rewrite the previous code by BitVector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's rewrite the code by Channels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Chennel itself is not distributed over processes but works serially (currently in Julia 1.0). In order to make it parallel, we need to use RemoteChannel instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RemoteChannel is not iterable, so partially I used Channel to sample from each distribution. This is why I exchanged the states instead of the temperatures. Processes are interacting via RemoteChannel and sampling is done inside each process using Channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
